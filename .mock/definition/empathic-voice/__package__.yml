errors:
  BadRequestError:
    status-code: 400
    type: ErrorResponse
    docs: Bad Request
    examples:
      - value: {}
types:
  ErrorResponse:
    properties:
      error: optional<string>
      message: optional<string>
    source:
      openapi: stenographer-openapi.json
  ReturnUserDefinedToolToolType:
    enum:
      - BUILTIN
      - FUNCTION
    docs: >-
      Type of Tool. Either `BUILTIN` for natively implemented tools, like web
      search, or `FUNCTION` for user-defined tools.
    source:
      openapi: stenographer-openapi.json
  ReturnUserDefinedToolVersionType:
    enum:
      - FIXED
      - LATEST
    docs: >-
      Versioning method for a Tool. Either `FIXED` for using a fixed version
      number or `LATEST` for auto-updating to the latest version.
    source:
      openapi: stenographer-openapi.json
  ReturnUserDefinedTool:
    docs: A specific tool version returned from the server
    properties:
      tool_type:
        type: ReturnUserDefinedToolToolType
        docs: >-
          Type of Tool. Either `BUILTIN` for natively implemented tools, like
          web search, or `FUNCTION` for user-defined tools.
      id:
        type: string
        docs: Identifier for a Tool. Formatted as a UUID.
      version:
        type: integer
        docs: >-
          Version number for a Tool.


          Tools, Configs, Custom Voices, and Prompts are versioned. This
          versioning system supports iterative development, allowing you to
          progressively refine tools and revert to previous versions if needed.


          Version numbers are integer values representing different iterations
          of the Tool. Each update to the Tool increments its version number.
      version_type:
        type: ReturnUserDefinedToolVersionType
        docs: >-
          Versioning method for a Tool. Either `FIXED` for using a fixed version
          number or `LATEST` for auto-updating to the latest version.
      version_description:
        type: optional<string>
        docs: An optional description of the Tool version.
      name:
        type: string
        docs: Name applied to all versions of a particular Tool.
      created_on:
        type: long
        docs: >-
          Time at which the Tool was created. Measured in seconds since the Unix
          epoch.
      modified_on:
        type: long
        docs: >-
          Time at which the Tool was last modified. Measured in seconds since
          the Unix epoch.
      fallback_content:
        type: optional<string>
        docs: >-
          Optional text passed to the supplemental LLM in place of the tool call
          result. The LLM then uses this text to generate a response back to the
          user, ensuring continuity in the conversation if the Tool errors.
      description:
        type: optional<string>
        docs: >-
          An optional description of what the Tool does, used by the
          supplemental LLM to choose when and how to call the function.
      parameters:
        type: string
        docs: >-
          Stringified JSON defining the parameters used by this version of the
          Tool.


          These parameters define the inputs needed for the Tool’s execution,
          including the expected data type and description for each input field.
          Structured as a stringified JSON schema, this format ensures the tool
          receives data in the expected format.
    source:
      openapi: stenographer-openapi.json
  ReturnPromptVersionType:
    enum:
      - FIXED
      - LATEST
    docs: >-
      Versioning method for a Prompt. Either `FIXED` for using a fixed version
      number or `LATEST` for auto-updating to the latest version.
    source:
      openapi: stenographer-openapi.json
  ReturnPrompt:
    docs: A Prompt associated with this Config.
    properties:
      id:
        type: string
        docs: Identifier for a Prompt. Formatted as a UUID.
      version:
        type: integer
        docs: >-
          Version number for a Prompt.


          Prompts, Configs, Custom Voices, and Tools are versioned. This
          versioning system supports iterative development, allowing you to
          progressively refine prompts and revert to previous versions if
          needed.


          Version numbers are integer values representing different iterations
          of the Prompt. Each update to the Prompt increments its version
          number.
      version_type:
        type: ReturnPromptVersionType
        docs: >-
          Versioning method for a Prompt. Either `FIXED` for using a fixed
          version number or `LATEST` for auto-updating to the latest version.
      version_description:
        type: optional<string>
        docs: An optional description of the Prompt version.
      name:
        type: string
        docs: Name applied to all versions of a particular Prompt.
      created_on:
        type: long
        docs: >-
          Time at which the Prompt was created. Measured in seconds since the
          Unix epoch.
      modified_on:
        type: long
        docs: >-
          Time at which the Prompt was last modified. Measured in seconds since
          the Unix epoch.
      text:
        type: string
        docs: >-
          Instructions used to shape EVI’s behavior, responses, and style.


          You can use the Prompt to define a specific goal or role for EVI,
          specifying how it should act or what it should focus on during the
          conversation. For example, EVI can be instructed to act as a customer
          support representative, a fitness coach, or a travel advisor, each
          with its own set of behaviors and response styles.


          For help writing a system prompt, see our [Prompting
          Guide](/docs/empathic-voice-interface-evi/prompting).
    source:
      openapi: stenographer-openapi.json
  PostedCustomVoiceBaseVoice:
    enum:
      - ITO
      - KORA
      - DACHER
      - AURA
      - FINN
      - WHIMSY
      - STELLA
      - SUNNY
    docs: Specifies the base voice used to create the Custom Voice.
    source:
      openapi: stenographer-openapi.json
  PostedCustomVoiceParameters:
    docs: >-
      The specified attributes of a Custom Voice. 


      If no parameters are specified then all attributes will be set to their
      defaults, meaning no modfications will be made to the base voice.
    properties:
      gender:
        type: optional<integer>
        docs: >-
          The perceived tonality of the voice, reflecting characteristics
          typically associated with masculinity and femininity.


          The default value is `0`, with a minimum of `-100` (more masculine)
          and a maximum of `100` (more feminine). A value of `0` leaves this
          parameter unchanged from the base voice.
      assertiveness:
        type: optional<integer>
        docs: >-
          The perceived firmness of the voice, ranging between whiny and bold.


          The default value is `0`, with a minimum of `-100` (whiny) and a
          maximum of `100` (bold). A value of `0` leaves this parameter
          unchanged from the base voice.
      buoyancy:
        type: optional<integer>
        docs: >-
          The perceived density of the voice, ranging between deflated and
          buoyant.


          The default value is `0`, with a minimum of `-100` (deflated) and a
          maximum of `100` (buoyant). A value of `0` leaves this parameter
          unchanged from the base voice.
      confidence:
        type: optional<integer>
        docs: >-
          The perceived assuredness of the voice, ranging between shy and
          confident.


          The default value is `0`, with a minimum of `-100` (shy) and a maximum
          of `100` (confident). A value of `0` leaves this parameter unchanged
          from the base voice.
      enthusiasm:
        type: optional<integer>
        docs: >-
          The perceived excitement within the voice, ranging between calm and
          enthusiastic.


          The default value is `0`, with a minimum of `-100` (calm) and a
          maximum of `100` (enthusiastic). A value of `0` leaves this parameter
          unchanged from the base voice.
      nasality:
        type: optional<integer>
        docs: >-
          The perceived openness of the voice, ranging between clear and nasal.


          The default value is `0`, with a minimum of `-100` (clear) and a
          maximum of `100` (nasal). A value of `0` leaves this parameter
          unchanged from the base voice.
      relaxedness:
        type: optional<integer>
        docs: >-
          The perceived stress within the voice, ranging between tense and
          relaxed.


          The default value is `0`, with a minimum of `-100` (tense) and a
          maximum of `100` (relaxed). A value of `0` leaves this parameter
          unchanged from the base voice.
      smoothness:
        type: optional<integer>
        docs: >-
          The perceived texture of the voice, ranging between smooth and
          staccato.


          The default value is `0`, with a minimum of `-100` (smooth) and a
          maximum of `100` (staccato). A value of `0` leaves this parameter
          unchanged from the base voice.
      tepidity:
        type: optional<integer>
        docs: >-
          The perceived liveliness behind the voice, ranging between tepid and
          vigorous.


          The default value is `0`, with a minimum of `-100` (tepid) and a
          maximum of `100` (vigorous). A value of `0` leaves this parameter
          unchanged from the base voice.
      tightness:
        type: optional<integer>
        docs: >-
          The perceived containment of the voice, ranging between tight and
          breathy.


          The default value is `0`, with a minimum of `-100` (tight) and a
          maximum of `100` (breathy). A value of `0` leaves this parameter
          unchanged from the base voice.
    source:
      openapi: stenographer-openapi.json
    inline: true
  PostedCustomVoice:
    docs: >-
      A Custom Voice specification to be associated with this Config.


      If a Custom Voice specification is not provided then the
      [name](/reference/empathic-voice-interface-evi/configs/create-config#request.body.voice.name)
      of a base voice or previously created Custom Voice must be provided.

       See our [Voices guide](/docs/empathic-voice-interface-evi/voices) for a tutorial on how to craft a Custom Voice.
    properties:
      name:
        type: string
        docs: >-
          The name of the Custom Voice. Maximum length of 75 characters. Will be
          converted to all-uppercase. (e.g., "sample voice" becomes "SAMPLE
          VOICE")
      base_voice:
        type: PostedCustomVoiceBaseVoice
        docs: Specifies the base voice used to create the Custom Voice.
      parameter_model:
        type: literal<"20241004-11parameter">
        docs: >-
          The name of the parameter model used to define which attributes are
          used by the `parameters` field. Currently, only `20241004-11parameter`
          is supported as the parameter model.
      parameters:
        type: optional<PostedCustomVoiceParameters>
        docs: >-
          The specified attributes of a Custom Voice. 


          If no parameters are specified then all attributes will be set to
          their defaults, meaning no modfications will be made to the base
          voice.
    source:
      openapi: stenographer-openapi.json
  ReturnCustomVoiceBaseVoice:
    enum:
      - ITO
      - KORA
      - DACHER
      - AURA
      - FINN
      - WHIMSY
      - STELLA
      - SUNNY
    docs: The base voice used to create the Custom Voice.
    source:
      openapi: stenographer-openapi.json
  ReturnCustomVoiceParameters:
    docs: >-
      The specified attributes of a Custom Voice. If a parameter's value is `0`
      (default), it will not be included in the response.
    properties:
      gender:
        type: optional<integer>
        docs: >-
          The perceived tonality of the voice, reflecting characteristics
          typically associated with masculinity and femininity.


          The default value is `0`, with a minimum of `-100` (more masculine)
          and a maximum of `100` (more feminine). A value of `0` leaves this
          parameter unchanged from the base voice.
      assertiveness:
        type: optional<integer>
        docs: >-
          The perceived firmness of the voice, ranging between whiny and bold.


          The default value is `0`, with a minimum of `-100` (whiny) and a
          maximum of `100` (bold). A value of `0` leaves this parameter
          unchanged from the base voice.
      buoyancy:
        type: optional<integer>
        docs: >-
          The perceived density of the voice, ranging between deflated and
          buoyant.


          The default value is `0`, with a minimum of `-100` (deflated) and a
          maximum of `100` (buoyant). A value of `0` leaves this parameter
          unchanged from the base voice.
      confidence:
        type: optional<integer>
        docs: >-
          The perceived assuredness of the voice, ranging between shy and
          confident.


          The default value is `0`, with a minimum of `-100` (shy) and a maximum
          of `100` (confident). A value of `0` leaves this parameter unchanged
          from the base voice.
      enthusiasm:
        type: optional<integer>
        docs: >-
          The perceived excitement within the voice, ranging between calm and
          enthusiastic.


          The default value is `0`, with a minimum of `-100` (calm) and a
          maximum of `100` (enthusiastic). A value of `0` leaves this parameter
          unchanged from the base voice.
      nasality:
        type: optional<integer>
        docs: >-
          The perceived openness of the voice, ranging between clear and nasal.


          The default value is `0`, with a minimum of `-100` (clear) and a
          maximum of `100` (nasal). A value of `0` leaves this parameter
          unchanged from the base voice.
      relaxedness:
        type: optional<integer>
        docs: >-
          The perceived stress within the voice, ranging between tense and
          relaxed.


          The default value is `0`, with a minimum of `-100` (tense) and a
          maximum of `100` (relaxed). A value of `0` leaves this parameter
          unchanged from the base voice.
      smoothness:
        type: optional<integer>
        docs: >-
          The perceived texture of the voice, ranging between smooth and
          staccato.


          The default value is `0`, with a minimum of `-100` (smooth) and a
          maximum of `100` (staccato). A value of `0` leaves this parameter
          unchanged from the base voice.
      tepidity:
        type: optional<integer>
        docs: >-
          The perceived liveliness behind the voice, ranging between tepid and
          vigorous.


          The default value is `0`, with a minimum of `-100` (tepid) and a
          maximum of `100` (vigorous). A value of `0` leaves this parameter
          unchanged from the base voice.
      tightness:
        type: optional<integer>
        docs: >-
          The perceived containment of the voice, ranging between tight and
          breathy.


          The default value is `0`, with a minimum of `-100` (tight) and a
          maximum of `100` (breathy). A value of `0` leaves this parameter
          unchanged from the base voice.
    source:
      openapi: stenographer-openapi.json
    inline: true
  ReturnCustomVoice:
    docs: A Custom Voice specification associated with this Config.
    properties:
      id:
        type: string
        docs: Identifier for a Custom Voice. Formatted as a UUID.
      version:
        type: integer
        docs: >-
          Version number for a Custom Voice.


          Custom Voices, Prompts, Configs, and Tools are versioned. This
          versioning system supports iterative development, allowing you to
          progressively refine configurations and revert to previous versions if
          needed.


          Version numbers are integer values representing different iterations
          of the Custom Voice. Each update to the Custom Voice increments its
          version number.
      name:
        type: string
        docs: The name of the Custom Voice. Maximum length of 75 characters.
      created_on:
        type: long
        docs: >-
          Time at which the Custom Voice was created. Measured in seconds since
          the Unix epoch.
      modified_on:
        type: long
        docs: >-
          Time at which the Custom Voice was last modified. Measured in seconds
          since the Unix epoch.
      base_voice:
        type: ReturnCustomVoiceBaseVoice
        docs: The base voice used to create the Custom Voice.
      parameter_model:
        type: literal<"20241004-11parameter">
        docs: >-
          The name of the parameter model used to define which attributes are
          used by the `parameters` field. Currently, only `20241004-11parameter`
          is supported as the parameter model.
      parameters:
        type: ReturnCustomVoiceParameters
        docs: >-
          The specified attributes of a Custom Voice. If a parameter's value is
          `0` (default), it will not be included in the response.
    source:
      openapi: stenographer-openapi.json
  PostedBuiltinToolName:
    enum:
      - web_search
      - hang_up
    docs: >-
      Name of the built-in tool to use. Hume supports the following built-in
      tools:


      - **web_search:** enables EVI to search the web for up-to-date information
      when applicable.

      - **hang_up:** closes the WebSocket connection when appropriate (e.g.,
      after detecting a farewell in the conversation).


      For more information, see our guide on [using built-in
      tools](/docs/empathic-voice-interface-evi/tool-use#using-built-in-tools).
    source:
      openapi: stenographer-openapi.json
  PostedBuiltinTool:
    docs: A configuration of a built-in tool to be posted to the server
    properties:
      name:
        type: PostedBuiltinToolName
        docs: >-
          Name of the built-in tool to use. Hume supports the following built-in
          tools:


          - **web_search:** enables EVI to search the web for up-to-date
          information when applicable.

          - **hang_up:** closes the WebSocket connection when appropriate (e.g.,
          after detecting a farewell in the conversation).


          For more information, see our guide on [using built-in
          tools](/docs/empathic-voice-interface-evi/tool-use#using-built-in-tools).
      fallback_content:
        type: optional<string>
        docs: >-
          Optional text passed to the supplemental LLM in place of the tool call
          result. The LLM then uses this text to generate a response back to the
          user, ensuring continuity in the conversation if the Tool errors.
    source:
      openapi: stenographer-openapi.json
  PostedConfigPromptSpec:
    docs: >-
      Identifies which prompt to use in a a config OR how to create a new prompt
      to use in the config
    properties:
      id:
        type: optional<string>
        docs: Identifier for a Prompt. Formatted as a UUID.
      version:
        type: optional<integer>
        docs: >-
          Version number for a Prompt. Version numbers should be integers. The
          combination of configId and version number is unique.
      text:
        type: optional<string>
        docs: Text used to create a new prompt for a particular config.
    source:
      openapi: stenographer-openapi.json
  PostedEllmModel:
    docs: A eLLM model configuration to be posted to the server
    properties:
      allow_short_responses:
        type: optional<boolean>
        docs: |-
          Boolean indicating if the eLLM is allowed to generate short responses.

          If omitted, short responses from the eLLM are enabled by default.
    source:
      openapi: stenographer-openapi.json
  PostedEventMessageSpec:
    docs: Settings for a specific event_message to be posted to the server
    properties:
      enabled:
        type: boolean
        docs: >-
          Boolean indicating if this event message is enabled.


          If set to `true`, a message will be sent when the circumstances for
          the specific event are met.
      text:
        type: optional<string>
        docs: >-
          Text to use as the event message when the corresponding event occurs.
          If no text is specified, EVI will generate an appropriate message
          based on its current context and the system prompt.
    source:
      openapi: stenographer-openapi.json
  PostedEventMessageSpecs:
    docs: >-
      Collection of event messages returned by the server.


      Event messages are sent by the server when specific events occur during a
      chat session. These messages are used to configure behaviors for EVI, such
      as controlling how EVI starts a new conversation.
    properties:
      on_new_chat:
        type: optional<PostedEventMessageSpec>
        docs: >-
          Specifies the initial message EVI provides when a new chat is started,
          such as a greeting or welcome message.
      on_inactivity_timeout:
        type: optional<PostedEventMessageSpec>
        docs: >-
          Specifies the message EVI provides when the chat is about to be
          disconnected due to a user inactivity timeout, such as a message
          mentioning a lack of user input for a period of time.


          Enabling an inactivity message allows developers to use this message
          event for "checking in" with the user if they are not responding to
          see if they are still active.


          If the user does not respond in the number of seconds specified in the
          `inactivity_timeout` field, then EVI will say the message and the user
          has 15 seconds to respond. If they respond in time, the conversation
          will continue; if not, the conversation will end.


          However, if the inactivity message is not enabled, then reaching the
          inactivity timeout will immediately end the connection.
      on_max_duration_timeout:
        type: optional<PostedEventMessageSpec>
        docs: >-
          Specifies the message EVI provides when the chat is disconnected due
          to reaching the maximum chat duration, such as a message mentioning
          the time limit for the chat has been reached.
    source:
      openapi: stenographer-openapi.json
  PostedLanguageModelModelProvider:
    enum:
      - OPEN_AI
      - CUSTOM_LANGUAGE_MODEL
      - ANTHROPIC
      - FIREWORKS
      - GROQ
      - GOOGLE
    docs: The provider of the supplemental language model.
    source:
      openapi: stenographer-openapi.json
  PostedLanguageModelModelResource:
    enum:
      - value: claude-3-5-sonnet-latest
        name: Claude35SonnetLatest
      - value: claude-3-5-sonnet-20240620
        name: Claude35Sonnet20240620
      - value: claude-3-opus-20240229
        name: Claude3Opus20240229
      - value: claude-3-sonnet-20240229
        name: Claude3Sonnet20240229
      - value: claude-3-haiku-20240307
        name: Claude3Haiku20240307
      - value: claude-2.1
        name: Claude21
      - value: claude-instant-1.2
        name: ClaudeInstant12
      - value: gemini-1.5-pro
        name: Gemini15Pro
      - value: gemini-1.5-flash
        name: Gemini15Flash
      - value: gemini-1.5-pro-002
        name: Gemini15Pro002
      - value: gemini-1.5-flash-002
        name: Gemini15Flash002
      - value: gpt-4-turbo-preview
        name: Gpt4TurboPreview
      - value: gpt-3.5-turbo-0125
        name: Gpt35Turbo0125
      - value: gpt-3.5-turbo
        name: Gpt35Turbo
      - value: gpt-4o
        name: Gpt4O
      - value: gpt-4o-mini
        name: Gpt4OMini
      - value: gemma-7b-it
        name: Gemma7BIt
      - value: llama3-8b-8192
        name: Llama38B8192
      - value: llama3-70b-8192
        name: Llama370B8192
      - value: llama-3.1-70b-versatile
        name: Llama3170BVersatile
      - value: llama-3.1-8b-instant
        name: Llama318BInstant
      - value: accounts/fireworks/models/mixtral-8x7b-instruct
        name: AccountsFireworksModelsMixtral8X7BInstruct
      - value: accounts/fireworks/models/llama-v3p1-405b-instruct
        name: AccountsFireworksModelsLlamaV3P1405BInstruct
      - value: accounts/fireworks/models/llama-v3p1-70b-instruct
        name: AccountsFireworksModelsLlamaV3P170BInstruct
      - value: accounts/fireworks/models/llama-v3p1-8b-instruct
        name: AccountsFireworksModelsLlamaV3P18BInstruct
      - ellm
    docs: String that specifies the language model to use with `model_provider`.
    source:
      openapi: stenographer-openapi.json
  PostedLanguageModel:
    docs: A LanguageModel to be posted to the server
    properties:
      model_provider:
        type: optional<PostedLanguageModelModelProvider>
        docs: The provider of the supplemental language model.
      model_resource:
        type: optional<PostedLanguageModelModelResource>
        docs: String that specifies the language model to use with `model_provider`.
      temperature:
        type: optional<float>
        docs: >-
          The model temperature, with values between 0 to 1 (inclusive).


          Controls the randomness of the LLM’s output, with values closer to 0
          yielding focused, deterministic responses and values closer to 1
          producing more creative, diverse responses.
    source:
      openapi: stenographer-openapi.json
  PostedTimeoutSpec:
    docs: Settings for a specific timeout to be posted to the server
    properties:
      enabled:
        type: boolean
        docs: Boolean indicating if this event message is enabled.
      duration_secs:
        type: optional<integer>
        docs: Duration in seconds for the timeout.
    source:
      openapi: stenographer-openapi.json
  PostedTimeoutSpecsInactivity:
    docs: >-
      Specifies the duration of user inactivity (in seconds) after which the EVI
      WebSocket connection will be automatically disconnected. Default is 600
      seconds (10 minutes).


      Accepts a minimum value of 30 seconds and a maximum value of 1,800
      seconds.
    properties:
      enabled:
        type: boolean
        docs: >-
          Boolean indicating if this timeout is enabled.


          If set to false, EVI will not timeout due to a specified duration of
          user inactivity being reached. However, the conversation will
          eventually disconnect after 1,800 seconds (30 minutes), which is the
          maximum WebSocket duration limit for EVI.
      duration_secs:
        type: optional<integer>
        docs: >-
          Duration in seconds for the timeout (e.g. 600 seconds represents 10
          minutes).
    source:
      openapi: stenographer-openapi.json
    inline: true
  PostedTimeoutSpecsMaxDuration:
    docs: >-
      Specifies the maximum allowed duration (in seconds) for an EVI WebSocket
      connection before it is automatically disconnected. Default is 1,800
      seconds (30 minutes).


      Accepts a minimum value of 30 seconds and a maximum value of 1,800
      seconds.
    properties:
      enabled:
        type: boolean
        docs: >-
          Boolean indicating if this timeout is enabled.


          If set to false, EVI will not timeout due to a specified maximum
          duration being reached. However, the conversation will eventually
          disconnect after 1,800 seconds (30 minutes), which is the maximum
          WebSocket duration limit for EVI.
      duration_secs:
        type: optional<integer>
        docs: >-
          Duration in seconds for the timeout (e.g. 600 seconds represents 10
          minutes).
    source:
      openapi: stenographer-openapi.json
    inline: true
  PostedTimeoutSpecs:
    docs: >-
      Collection of timeout specifications returned by the server.


      Timeouts are sent by the server when specific time-based events occur
      during a chat session. These specifications set the inactivity timeout and
      the maximum duration an EVI WebSocket connection can stay open before it
      is automatically disconnected.
    properties:
      inactivity:
        type: optional<PostedTimeoutSpecsInactivity>
        docs: >-
          Specifies the duration of user inactivity (in seconds) after which the
          EVI WebSocket connection will be automatically disconnected. Default
          is 600 seconds (10 minutes).


          Accepts a minimum value of 30 seconds and a maximum value of 1,800
          seconds.
      max_duration:
        type: optional<PostedTimeoutSpecsMaxDuration>
        docs: >-
          Specifies the maximum allowed duration (in seconds) for an EVI
          WebSocket connection before it is automatically disconnected. Default
          is 1,800 seconds (30 minutes).


          Accepts a minimum value of 30 seconds and a maximum value of 1,800
          seconds.
    source:
      openapi: stenographer-openapi.json
  PostedUserDefinedToolSpec:
    docs: A specific tool identifier to be posted to the server
    properties:
      id:
        type: string
        docs: Identifier for a Tool. Formatted as a UUID.
      version:
        type: optional<integer>
        docs: >-
          Version number for a Tool.


          Tools, Configs, Custom Voices, and Prompts are versioned. This
          versioning system supports iterative development, allowing you to
          progressively refine tools and revert to previous versions if needed.


          Version numbers are integer values representing different iterations
          of the Tool. Each update to the Tool increments its version number.
    source:
      openapi: stenographer-openapi.json
  PostedVoiceProvider:
    enum:
      - HUME_AI
      - CUSTOM_VOICE
    docs: >-
      The provider of the voice to use. Supported values are `HUME_AI` and
      `CUSTOM_VOICE`.
    source:
      openapi: stenographer-openapi.json
  PostedVoice:
    docs: A Voice specification posted to the server
    properties:
      provider:
        type: PostedVoiceProvider
        docs: >-
          The provider of the voice to use. Supported values are `HUME_AI` and
          `CUSTOM_VOICE`.
      name:
        type: optional<string>
        docs: >-
          Specifies the name of the voice to use.


          This can be either the name of a previously created Custom Voice or
          one of our 8 base voices: `ITO`, `KORA`, `DACHER`, `AURA`, `FINN`,
          `WHIMSY`, `STELLA`, or `SUNNY`.


          The name will be automatically converted to uppercase (e.g., "Ito"
          becomes "ITO"). If a name is not specified, then a [Custom
          Voice](/reference/empathic-voice-interface-evi/configs/create-config#request.body.voice.custom_voice)
          specification must be provided.
      custom_voice: optional<PostedCustomVoice>
    source:
      openapi: stenographer-openapi.json
  ReturnBuiltinToolToolType:
    enum:
      - BUILTIN
      - FUNCTION
    docs: >-
      Type of Tool. Either `BUILTIN` for natively implemented tools, like web
      search, or `FUNCTION` for user-defined tools.
    source:
      openapi: stenographer-openapi.json
  ReturnBuiltinTool:
    docs: A specific builtin tool version returned from the server
    properties:
      tool_type:
        type: ReturnBuiltinToolToolType
        docs: >-
          Type of Tool. Either `BUILTIN` for natively implemented tools, like
          web search, or `FUNCTION` for user-defined tools.
      name:
        type: string
        docs: Name applied to all versions of a particular Tool.
      fallback_content:
        type: optional<string>
        docs: >-
          Optional text passed to the supplemental LLM in place of the tool call
          result. The LLM then uses this text to generate a response back to the
          user, ensuring continuity in the conversation if the Tool errors.
    source:
      openapi: stenographer-openapi.json
  ReturnConfig:
    docs: A specific config version returned from the server
    properties:
      id:
        type: optional<string>
        docs: Identifier for a Config. Formatted as a UUID.
      version:
        type: optional<integer>
        docs: >-
          Version number for a Config.


          Configs, Prompts, Custom Voices, and Tools are versioned. This
          versioning system supports iterative development, allowing you to
          progressively refine configurations and revert to previous versions if
          needed.


          Version numbers are integer values representing different iterations
          of the Config. Each update to the Config increments its version
          number.
      evi_version:
        type: optional<string>
        docs: >-
          Specifies the EVI version to use. Use `"1"` for version 1, or `"2"`
          for the latest enhanced version. For a detailed comparison of the two
          versions, refer to our
          [guide](/docs/empathic-voice-interface-evi/evi-2).
      version_description:
        type: optional<string>
        docs: An optional description of the Config version.
      name:
        type: optional<string>
        docs: Name applied to all versions of a particular Config.
      created_on:
        type: optional<long>
        docs: >-
          Time at which the Config was created. Measured in seconds since the
          Unix epoch.
      modified_on:
        type: optional<long>
        docs: >-
          Time at which the Config was last modified. Measured in seconds since
          the Unix epoch.
      prompt: optional<ReturnPrompt>
      voice:
        type: optional<ReturnVoice>
        docs: A voice specification associated with this Config.
      language_model:
        type: optional<ReturnLanguageModel>
        docs: >-
          The supplemental language model associated with this Config.


          This model is used to generate longer, more detailed responses from
          EVI. Choosing an appropriate supplemental language model for your use
          case is crucial for generating fast, high-quality responses from EVI.
      ellm_model:
        type: optional<ReturnEllmModel>
        docs: >-
          The eLLM setup associated with this Config.


          Hume's eLLM (empathic Large Language Model) is a multimodal language
          model that takes into account both expression measures and language.
          The eLLM generates short, empathic language responses and guides
          text-to-speech (TTS) prosody.
      tools:
        type: optional<list<optional<ReturnUserDefinedTool>>>
        docs: List of user-defined tools associated with this Config.
      builtin_tools:
        type: optional<list<optional<ReturnBuiltinTool>>>
        docs: List of built-in tools associated with this Config.
      event_messages: optional<ReturnEventMessageSpecs>
      timeouts: optional<ReturnTimeoutSpecs>
    source:
      openapi: stenographer-openapi.json
  ReturnEllmModel:
    docs: A specific eLLM Model configuration
    properties:
      allow_short_responses:
        type: boolean
        docs: |-
          Boolean indicating if the eLLM is allowed to generate short responses.

          If omitted, short responses from the eLLM are enabled by default.
    source:
      openapi: stenographer-openapi.json
  ReturnEventMessageSpec:
    docs: A specific event message configuration to be returned from the server
    properties:
      enabled:
        type: boolean
        docs: >-
          Boolean indicating if this event message is enabled.


          If set to `true`, a message will be sent when the circumstances for
          the specific event are met.
      text:
        type: optional<string>
        docs: >-
          Text to use as the event message when the corresponding event occurs.
          If no text is specified, EVI will generate an appropriate message
          based on its current context and the system prompt.
    source:
      openapi: stenographer-openapi.json
  ReturnEventMessageSpecs:
    docs: >-
      Collection of event messages returned by the server.


      Event messages are sent by the server when specific events occur during a
      chat session. These messages are used to configure behaviors for EVI, such
      as controlling how EVI starts a new conversation.
    properties:
      on_new_chat:
        type: optional<ReturnEventMessageSpec>
        docs: >-
          Specifies the initial message EVI provides when a new chat is started,
          such as a greeting or welcome message.
      on_inactivity_timeout:
        type: optional<ReturnEventMessageSpec>
        docs: >-
          Specifies the message EVI provides when the chat is about to be
          disconnected due to a user inactivity timeout, such as a message
          mentioning a lack of user input for a period of time.


          Enabling an inactivity message allows developers to use this message
          event for "checking in" with the user if they are not responding to
          see if they are still active.


          If the user does not respond in the number of seconds specified in the
          `inactivity_timeout` field, then EVI will say the message and the user
          has 15 seconds to respond. If they respond in time, the conversation
          will continue; if not, the conversation will end.


          However, if the inactivity message is not enabled, then reaching the
          inactivity timeout will immediately end the connection.
      on_max_duration_timeout:
        type: optional<ReturnEventMessageSpec>
        docs: >-
          Specifies the message EVI provides when the chat is disconnected due
          to reaching the maximum chat duration, such as a message mentioning
          the time limit for the chat has been reached.
    source:
      openapi: stenographer-openapi.json
  ReturnLanguageModelModelProvider:
    enum:
      - OPEN_AI
      - CUSTOM_LANGUAGE_MODEL
      - ANTHROPIC
      - FIREWORKS
      - GROQ
      - GOOGLE
    docs: The provider of the supplemental language model.
    source:
      openapi: stenographer-openapi.json
  ReturnLanguageModelModelResource:
    enum:
      - value: claude-3-5-sonnet-latest
        name: Claude35SonnetLatest
      - value: claude-3-5-haiku-latest
        name: Claude35HaikuLatest
      - value: claude-3-5-sonnet-20240620
        name: Claude35Sonnet20240620
      - value: claude-3-5-haiku-20241022
        name: Claude35Haiku20241022
      - value: claude-3-opus-20240229
        name: Claude3Opus20240229
      - value: claude-3-sonnet-20240229
        name: Claude3Sonnet20240229
      - value: claude-3-haiku-20240307
        name: Claude3Haiku20240307
      - value: claude-2.1
        name: Claude21
      - value: claude-instant-1.2
        name: ClaudeInstant12
      - value: gemini-1.5-pro
        name: Gemini15Pro
      - value: gemini-1.5-flash
        name: Gemini15Flash
      - value: gemini-1.5-pro-002
        name: Gemini15Pro002
      - value: gemini-1.5-flash-002
        name: Gemini15Flash002
      - value: gpt-4-turbo-preview
        name: Gpt4TurboPreview
      - value: gpt-3.5-turbo-0125
        name: Gpt35Turbo0125
      - value: gpt-3.5-turbo
        name: Gpt35Turbo
      - value: gpt-4o
        name: Gpt4O
      - value: gpt-4o-mini
        name: Gpt4OMini
      - value: gemma-7b-it
        name: Gemma7BIt
      - value: llama3-8b-8192
        name: Llama38B8192
      - value: llama3-70b-8192
        name: Llama370B8192
      - value: llama-3.1-70b-versatile
        name: Llama3170BVersatile
      - value: llama-3.1-8b-instant
        name: Llama318BInstant
      - value: accounts/fireworks/models/mixtral-8x7b-instruct
        name: AccountsFireworksModelsMixtral8X7BInstruct
      - value: accounts/fireworks/models/llama-v3p1-405b-instruct
        name: AccountsFireworksModelsLlamaV3P1405BInstruct
      - value: accounts/fireworks/models/llama-v3p1-70b-instruct
        name: AccountsFireworksModelsLlamaV3P170BInstruct
      - value: accounts/fireworks/models/llama-v3p1-8b-instruct
        name: AccountsFireworksModelsLlamaV3P18BInstruct
      - ellm
    docs: String that specifies the language model to use with `model_provider`.
    source:
      openapi: stenographer-openapi.json
  ReturnLanguageModel:
    docs: A specific LanguageModel
    properties:
      model_provider:
        type: optional<ReturnLanguageModelModelProvider>
        docs: The provider of the supplemental language model.
      model_resource:
        type: optional<ReturnLanguageModelModelResource>
        docs: String that specifies the language model to use with `model_provider`.
      temperature:
        type: optional<float>
        docs: >-
          The model temperature, with values between 0 to 1 (inclusive).


          Controls the randomness of the LLM’s output, with values closer to 0
          yielding focused, deterministic responses and values closer to 1
          producing more creative, diverse responses.
    source:
      openapi: stenographer-openapi.json
  ReturnTimeoutSpec:
    docs: A specific timeout configuration to be returned from the server
    properties:
      enabled:
        type: boolean
        docs: >-
          Boolean indicating if this timeout is enabled.


          If set to false, EVI will not timeout due to a specified duration
          being reached. However, the conversation will eventually disconnect
          after 1,800 seconds (30 minutes), which is the maximum WebSocket
          duration limit for EVI.
      duration_secs:
        type: optional<integer>
        docs: >-
          Duration in seconds for the timeout (e.g. 600 seconds represents 10
          minutes).
    source:
      openapi: stenographer-openapi.json
  ReturnTimeoutSpecs:
    docs: >-
      Collection of timeout specifications returned by the server.


      Timeouts are sent by the server when specific time-based events occur
      during a chat session. These specifications set the inactivity timeout and
      the maximum duration an EVI WebSocket connection can stay open before it
      is automatically disconnected.
    properties:
      inactivity:
        type: ReturnTimeoutSpec
        docs: >-
          Specifies the duration of user inactivity (in seconds) after which the
          EVI WebSocket connection will be automatically disconnected. Default
          is 600 seconds (10 minutes).


          Accepts a minimum value of 30 seconds and a maximum value of 1,800
          seconds.
      max_duration:
        type: ReturnTimeoutSpec
        docs: >-
          Specifies the maximum allowed duration (in seconds) for an EVI
          WebSocket connection before it is automatically disconnected. Default
          is 1,800 seconds (30 minutes).


          Accepts a minimum value of 30 seconds and a maximum value of 1,800
          seconds.
    source:
      openapi: stenographer-openapi.json
  ReturnVoiceProvider:
    enum:
      - HUME_AI
      - CUSTOM_VOICE
    docs: >-
      The provider of the voice to use. Supported values are `HUME_AI` and
      `CUSTOM_VOICE`.
    source:
      openapi: stenographer-openapi.json
  ReturnVoice:
    docs: A specific voice specification
    properties:
      provider:
        type: ReturnVoiceProvider
        docs: >-
          The provider of the voice to use. Supported values are `HUME_AI` and
          `CUSTOM_VOICE`.
      name:
        type: optional<string>
        docs: >-
          The name of the specified voice.


          This will either be the name of a previously created Custom Voice or
          one of our 8 base voices: `ITO`, `KORA`, `DACHER`, `AURA`, `FINN`,
          `WHIMSY`, `STELLA`, or `SUNNY`.
      custom_voice: optional<ReturnCustomVoice>
    source:
      openapi: stenographer-openapi.json
  ReturnPagedUserDefinedTools:
    docs: A paginated list of user defined tool versions returned from the server
    properties:
      page_number:
        type: integer
        docs: >-
          The page number of the returned list.


          This value corresponds to the `page_number` parameter specified in the
          request. Pagination uses zero-based indexing.
      page_size:
        type: integer
        docs: >-
          The maximum number of items returned per page.


          This value corresponds to the `page_size` parameter specified in the
          request.
      total_pages:
        type: integer
        docs: The total number of pages in the collection.
      tools_page:
        docs: >-
          List of tools returned for the specified `page_number` and
          `page_size`.
        type: list<optional<ReturnUserDefinedTool>>
    source:
      openapi: stenographer-openapi.json
  ReturnPagedPrompts:
    docs: A paginated list of prompt versions returned from the server
    properties:
      page_number:
        type: integer
        docs: >-
          The page number of the returned list.


          This value corresponds to the `page_number` parameter specified in the
          request. Pagination uses zero-based indexing.
      page_size:
        type: integer
        docs: >-
          The maximum number of items returned per page.


          This value corresponds to the `page_size` parameter specified in the
          request.
      total_pages:
        type: integer
        docs: The total number of pages in the collection.
      prompts_page:
        docs: >-
          List of prompts returned for the specified `page_number` and
          `page_size`.
        type: list<optional<ReturnPrompt>>
    source:
      openapi: stenographer-openapi.json
  ReturnPagedCustomVoices:
    docs: A paginated list of custom voices returned from the server
    properties:
      page_number:
        type: integer
        docs: >-
          The page number of the returned list.


          This value corresponds to the `page_number` parameter specified in the
          request. Pagination uses zero-based indexing.
      page_size:
        type: integer
        docs: >-
          The maximum number of items returned per page.


          This value corresponds to the `page_size` parameter specified in the
          request.
      total_pages:
        type: integer
        docs: The total number of pages in the collection.
      custom_voices_page:
        docs: List of Custom Voices for the specified `page_number` and `page_size`.
        type: list<ReturnCustomVoice>
    source:
      openapi: stenographer-openapi.json
  ReturnPagedConfigs:
    docs: A paginated list of config versions returned from the server
    properties:
      page_number:
        type: optional<integer>
        docs: >-
          The page number of the returned list.


          This value corresponds to the `page_number` parameter specified in the
          request. Pagination uses zero-based indexing.
      page_size:
        type: optional<integer>
        docs: >-
          The maximum number of items returned per page.


          This value corresponds to the `page_size` parameter specified in the
          request.
      total_pages:
        type: integer
        docs: The total number of pages in the collection.
      configs_page:
        type: optional<list<ReturnConfig>>
        docs: >-
          List of configs returned for the specified `page_number` and
          `page_size`.
    source:
      openapi: stenographer-openapi.json
  ReturnChatStatus:
    enum:
      - ACTIVE
      - USER_ENDED
      - USER_TIMEOUT
      - MAX_DURATION_TIMEOUT
      - INACTIVITY_TIMEOUT
      - ERROR
    docs: >-
      Indicates the current state of the chat. There are six possible statuses:


      - `ACTIVE`: The chat is currently active and ongoing.


      - `USER_ENDED`: The chat was manually ended by the user.


      - `USER_TIMEOUT`: The chat ended due to a user-defined timeout.


      - `MAX_DURATION_TIMEOUT`: The chat ended because it reached the maximum
      allowed duration.


      - `INACTIVITY_TIMEOUT`: The chat ended due to an inactivity timeout.


      - `ERROR`: The chat ended unexpectedly due to an error.
    source:
      openapi: stenographer-openapi.json
  ReturnChat:
    docs: A description of chat and its status
    properties:
      id:
        type: string
        docs: Identifier for a Chat. Formatted as a UUID.
      chat_group_id:
        type: string
        docs: >-
          Identifier for the Chat Group. Any chat resumed from this Chat will
          have the same `chat_group_id`. Formatted as a UUID.
      status:
        type: ReturnChatStatus
        docs: >-
          Indicates the current state of the chat. There are six possible
          statuses:


          - `ACTIVE`: The chat is currently active and ongoing.


          - `USER_ENDED`: The chat was manually ended by the user.


          - `USER_TIMEOUT`: The chat ended due to a user-defined timeout.


          - `MAX_DURATION_TIMEOUT`: The chat ended because it reached the
          maximum allowed duration.


          - `INACTIVITY_TIMEOUT`: The chat ended due to an inactivity timeout.


          - `ERROR`: The chat ended unexpectedly due to an error.
      start_timestamp:
        type: long
        docs: >-
          Time at which the Chat started. Measured in seconds since the Unix
          epoch.
      end_timestamp:
        type: optional<long>
        docs: >-
          Time at which the Chat ended. Measured in seconds since the Unix
          epoch.
      event_count:
        type: optional<long>
        docs: The total number of events currently in this chat.
      metadata:
        type: optional<string>
        docs: Stringified JSON with additional metadata about the chat.
      config: optional<ReturnConfigSpec>
    source:
      openapi: stenographer-openapi.json
  ReturnConfigSpec:
    docs: The Config associated with this Chat.
    properties:
      id:
        type: string
        docs: Identifier for a Config. Formatted as a UUID.
      version:
        type: optional<integer>
        docs: >-
          Version number for a Config.


          Configs, Prompts, Custom Voices, and Tools are versioned. This
          versioning system supports iterative development, allowing you to
          progressively refine configurations and revert to previous versions if
          needed.


          Version numbers are integer values representing different iterations
          of the Config. Each update to the Config increments its version
          number.
    source:
      openapi: stenographer-openapi.json
  ReturnPagedChatsPaginationDirection:
    enum:
      - ASC
      - DESC
    docs: >-
      Indicates the order in which the paginated results are presented, based on
      their creation date.


      It shows `ASC` for ascending order (chronological, with the oldest records
      first) or `DESC` for descending order (reverse-chronological, with the
      newest records first). This value corresponds to the `ascending_order`
      query parameter used in the request.
    source:
      openapi: stenographer-openapi.json
  ReturnPagedChats:
    docs: A paginated list of chats returned from the server
    properties:
      page_number:
        type: integer
        docs: >-
          The page number of the returned list.


          This value corresponds to the `page_number` parameter specified in the
          request. Pagination uses zero-based indexing.
      page_size:
        type: integer
        docs: >-
          The maximum number of items returned per page.


          This value corresponds to the `page_size` parameter specified in the
          request.
      total_pages:
        type: integer
        docs: The total number of pages in the collection.
      pagination_direction:
        type: ReturnPagedChatsPaginationDirection
        docs: >-
          Indicates the order in which the paginated results are presented,
          based on their creation date.


          It shows `ASC` for ascending order (chronological, with the oldest
          records first) or `DESC` for descending order (reverse-chronological,
          with the newest records first). This value corresponds to the
          `ascending_order` query parameter used in the request.
      chats_page:
        docs: >-
          List of Chats and their metadata returned for the specified
          `page_number` and `page_size`.
        type: list<ReturnChat>
    source:
      openapi: stenographer-openapi.json
  ReturnChatEventRole:
    enum:
      - USER
      - AGENT
      - SYSTEM
      - TOOL
    docs: >-
      The role of the entity which generated the Chat Event. There are four
      possible values:


      - `USER`: The user, capable of sending user messages and interruptions.


      - `AGENT`: The assistant, capable of sending agent messages.


      - `SYSTEM`: The backend server, capable of transmitting errors.


      - `TOOL`: The function calling mechanism.
    source:
      openapi: stenographer-openapi.json
  ReturnChatEventType:
    enum:
      - SYSTEM_PROMPT
      - USER_MESSAGE
      - USER_INTERRUPTION
      - AGENT_MESSAGE
      - FUNCTION_CALL
      - FUNCTION_CALL_RESPONSE
    docs: >-
      Type of Chat Event. There are six possible values:


      - `SYSTEM_PROMPT`: Contains the system prompt for use in the session.


      - `USER_MESSAGE`: Contains the message sent by the user.


      - `USER_INTERRUPTION`: Contains an interruption made by the user while the
      agent is speaking.


      - `AGENT_MESSAGE`: Contains the assistant’s message, generated by Hume’s
      eLLM and supplemental LLM.


      - `FUNCTION_CALL`: Contains the invocation of a tool.


      - `FUNCTION_CALL_RESPONSE`: Contains the tool response.
    source:
      openapi: stenographer-openapi.json
  ReturnChatEvent:
    docs: A description of a single event in a chat returned from the server
    properties:
      id:
        type: string
        docs: Identifier for a Chat Event. Formatted as a UUID.
      chat_id:
        type: string
        docs: Identifier for the Chat this event occurred in. Formatted as a UUID.
      timestamp:
        type: long
        docs: >-
          Time at which the Chat Event occurred. Measured in seconds since the
          Unix epoch.
      role:
        type: ReturnChatEventRole
        docs: >-
          The role of the entity which generated the Chat Event. There are four
          possible values:


          - `USER`: The user, capable of sending user messages and
          interruptions.


          - `AGENT`: The assistant, capable of sending agent messages.


          - `SYSTEM`: The backend server, capable of transmitting errors.


          - `TOOL`: The function calling mechanism.
      type:
        type: ReturnChatEventType
        docs: >-
          Type of Chat Event. There are six possible values:


          - `SYSTEM_PROMPT`: Contains the system prompt for use in the session.


          - `USER_MESSAGE`: Contains the message sent by the user.


          - `USER_INTERRUPTION`: Contains an interruption made by the user while
          the agent is speaking.


          - `AGENT_MESSAGE`: Contains the assistant’s message, generated by
          Hume’s eLLM and supplemental LLM.


          - `FUNCTION_CALL`: Contains the invocation of a tool.


          - `FUNCTION_CALL_RESPONSE`: Contains the tool response.
      message_text:
        type: optional<string>
        docs: >-
          The text of the Chat Event. This field contains the message content
          for each event type listed in the `type` field.
      emotion_features:
        type: optional<string>
        docs: >-
          Stringified JSON containing the prosody model inference results.


          EVI uses the prosody model to measure 48 expressions related to speech
          and vocal characteristics. These results contain a detailed emotional
          and tonal analysis of the audio. Scores typically range from 0 to 1,
          with higher values indicating a stronger confidence level in the
          measured attribute.
      metadata:
        type: optional<string>
        docs: Stringified JSON with additional metadata about the chat event.
    source:
      openapi: stenographer-openapi.json
  ReturnChatPagedEventsStatus:
    enum:
      - ACTIVE
      - USER_ENDED
      - USER_TIMEOUT
      - MAX_DURATION_TIMEOUT
      - INACTIVITY_TIMEOUT
      - ERROR
    docs: >-
      Indicates the current state of the chat. There are six possible statuses:


      - `ACTIVE`: The chat is currently active and ongoing.


      - `USER_ENDED`: The chat was manually ended by the user.


      - `USER_TIMEOUT`: The chat ended due to a user-defined timeout.


      - `MAX_DURATION_TIMEOUT`: The chat ended because it reached the maximum
      allowed duration.


      - `INACTIVITY_TIMEOUT`: The chat ended due to an inactivity timeout.


      - `ERROR`: The chat ended unexpectedly due to an error.
    source:
      openapi: stenographer-openapi.json
  ReturnChatPagedEventsPaginationDirection:
    enum:
      - ASC
      - DESC
    docs: >-
      Indicates the order in which the paginated results are presented, based on
      their creation date.


      It shows `ASC` for ascending order (chronological, with the oldest records
      first) or `DESC` for descending order (reverse-chronological, with the
      newest records first). This value corresponds to the `ascending_order`
      query parameter used in the request.
    source:
      openapi: stenographer-openapi.json
  ReturnChatPagedEvents:
    docs: >-
      A description of chat status with a paginated list of chat events returned
      from the server
    properties:
      id:
        type: string
        docs: Identifier for a Chat. Formatted as a UUID.
      chat_group_id:
        type: string
        docs: >-
          Identifier for the Chat Group. Any chat resumed from this Chat will
          have the same `chat_group_id`. Formatted as a UUID.
      status:
        type: ReturnChatPagedEventsStatus
        docs: >-
          Indicates the current state of the chat. There are six possible
          statuses:


          - `ACTIVE`: The chat is currently active and ongoing.


          - `USER_ENDED`: The chat was manually ended by the user.


          - `USER_TIMEOUT`: The chat ended due to a user-defined timeout.


          - `MAX_DURATION_TIMEOUT`: The chat ended because it reached the
          maximum allowed duration.


          - `INACTIVITY_TIMEOUT`: The chat ended due to an inactivity timeout.


          - `ERROR`: The chat ended unexpectedly due to an error.
      start_timestamp:
        type: long
        docs: >-
          Time at which the Chat started. Measured in seconds since the Unix
          epoch.
      end_timestamp:
        type: optional<long>
        docs: >-
          Time at which the Chat ended. Measured in seconds since the Unix
          epoch.
      pagination_direction:
        type: ReturnChatPagedEventsPaginationDirection
        docs: >-
          Indicates the order in which the paginated results are presented,
          based on their creation date.


          It shows `ASC` for ascending order (chronological, with the oldest
          records first) or `DESC` for descending order (reverse-chronological,
          with the newest records first). This value corresponds to the
          `ascending_order` query parameter used in the request.
      events_page:
        docs: List of Chat Events for the specified `page_number` and `page_size`.
        type: list<ReturnChatEvent>
      metadata:
        type: optional<string>
        docs: Stringified JSON with additional metadata about the chat.
      page_number:
        type: integer
        docs: >-
          The page number of the returned list.


          This value corresponds to the `page_number` parameter specified in the
          request. Pagination uses zero-based indexing.
      page_size:
        type: integer
        docs: >-
          The maximum number of items returned per page.


          This value corresponds to the `page_size` parameter specified in the
          request.
      total_pages:
        type: integer
        docs: The total number of pages in the collection.
      config: optional<ReturnConfigSpec>
    source:
      openapi: stenographer-openapi.json
  ReturnChatAudioReconstructionStatus:
    enum:
      - QUEUED
      - IN_PROGRESS
      - COMPLETE
      - ERROR
      - CANCELLED
    docs: >-
      Indicates the current state of the audio reconstruction job. There are
      five possible statuses:


      - `QUEUED`: The reconstruction job is waiting to be processed.


      - `IN_PROGRESS`: The reconstruction is currently being processed.


      - `COMPLETE`: The audio reconstruction is finished and ready for download.


      - `ERROR`: An error occurred during the reconstruction process.


      - `CANCELED`: The reconstruction job has been canceled.
    source:
      openapi: stenographer-openapi.json
  ReturnChatAudioReconstruction:
    docs: >-
      List of chat audio reconstructions returned for the specified page number
      and page size.
    properties:
      id:
        type: string
        docs: Identifier for the chat. Formatted as a UUID.
      user_id:
        type: string
        docs: Identifier for the user that owns this chat. Formatted as a UUID.
      status:
        type: ReturnChatAudioReconstructionStatus
        docs: >-
          Indicates the current state of the audio reconstruction job. There are
          five possible statuses:


          - `QUEUED`: The reconstruction job is waiting to be processed.


          - `IN_PROGRESS`: The reconstruction is currently being processed.


          - `COMPLETE`: The audio reconstruction is finished and ready for
          download.


          - `ERROR`: An error occurred during the reconstruction process.


          - `CANCELED`: The reconstruction job has been canceled.
      filename:
        type: optional<string>
        docs: Name of the chat audio reconstruction file.
      modified_at:
        type: optional<long>
        docs: >-
          The timestamp of the most recent status change for this audio
          reconstruction, formatted milliseconds since the Unix epoch.
      signed_audio_url:
        type: optional<string>
        docs: Signed URL used to download the chat audio reconstruction file.
      signed_url_expiration_timestamp_millis:
        type: optional<long>
        docs: >-
          The timestamp when the signed URL will expire, formatted as a Unix
          epoch milliseconds.
    source:
      openapi: stenographer-openapi.json
  ReturnActiveChatCount:
    docs: A description of current chat chat sessions for a user
    properties:
      timestamp:
        type: long
        docs: >-
          The timestamp for when chat status was measured. Formatted as a Unix
          epoch milliseconds.
      total_user_active_chats:
        type: integer
        docs: The total number of active chats for this user.
      max_allowed_active_chats:
        type: optional<integer>
        docs: The maximum number of concurrent active chats for this user.
      more_active_chats_allowed:
        type: boolean
        docs: Boolean indicating if the user is allowed to start more chats.
      per_tag:
        type: optional<list<optional<ReturnActiveChatCountPerTag>>>
        docs: Optional List of chat counts per tag.
    source:
      openapi: stenographer-openapi.json
  ReturnActiveChatCountPerTag:
    docs: A description of current chat chat sessions per tag
    properties:
      tag:
        type: string
        docs: User tag applied to a chat.
      total_tag_active_chats:
        type: integer
        docs: The total number of active chats for this user with the specified tag.
    source:
      openapi: stenographer-openapi.json
  ReturnChatGroup:
    docs: A description of chat_group and its status
    properties:
      id:
        type: string
        docs: >-
          Identifier for the Chat Group. Any Chat resumed from this Chat Group
          will have the same `chat_group_id`. Formatted as a UUID.
      first_start_timestamp:
        type: long
        docs: >-
          Time at which the first Chat in this Chat Group was created. Measured
          in seconds since the Unix epoch.
      most_recent_start_timestamp:
        type: long
        docs: >-
          Time at which the most recent Chat in this Chat Group was created.
          Measured in seconds since the Unix epoch.
      most_recent_chat_id:
        type: optional<string>
        docs: >-
          The `chat_id` of the most recent Chat in this Chat Group. Formatted as
          a UUID.
      num_chats:
        type: integer
        docs: The total number of Chats in this Chat Group.
      active:
        type: optional<boolean>
        docs: >-
          Denotes whether there is an active Chat associated with this Chat
          Group.
    source:
      openapi: stenographer-openapi.json
  ReturnPagedChatGroupsPaginationDirection:
    enum:
      - ASC
      - DESC
    docs: >-
      Indicates the order in which the paginated results are presented, based on
      their creation date.


      It shows `ASC` for ascending order (chronological, with the oldest records
      first) or `DESC` for descending order (reverse-chronological, with the
      newest records first). This value corresponds to the `ascending_order`
      query parameter used in the request.
    source:
      openapi: stenographer-openapi.json
  ReturnPagedChatGroups:
    docs: A paginated list of chat_groups returned from the server
    properties:
      page_number:
        type: integer
        docs: >-
          The page number of the returned list.


          This value corresponds to the `page_number` parameter specified in the
          request. Pagination uses zero-based indexing.
      page_size:
        type: integer
        docs: >-
          The maximum number of items returned per page.


          This value corresponds to the `page_size` parameter specified in the
          request.
      total_pages:
        type: integer
        docs: The total number of pages in the collection.
      pagination_direction:
        type: ReturnPagedChatGroupsPaginationDirection
        docs: >-
          Indicates the order in which the paginated results are presented,
          based on their creation date.


          It shows `ASC` for ascending order (chronological, with the oldest
          records first) or `DESC` for descending order (reverse-chronological,
          with the newest records first). This value corresponds to the
          `ascending_order` query parameter used in the request.
      chat_groups_page:
        docs: >-
          List of Chat Groups and their metadata returned for the specified
          `page_number` and `page_size`.
        type: list<ReturnChatGroup>
    source:
      openapi: stenographer-openapi.json
  ReturnChatGroupPagedChatsPaginationDirection:
    enum:
      - ASC
      - DESC
    docs: >-
      Indicates the order in which the paginated results are presented, based on
      their creation date.


      It shows `ASC` for ascending order (chronological, with the oldest records
      first) or `DESC` for descending order (reverse-chronological, with the
      newest records first). This value corresponds to the `ascending_order`
      query parameter used in the request.
    source:
      openapi: stenographer-openapi.json
  ReturnChatGroupPagedChats:
    docs: >-
      A description of chat_group and its status with a paginated list of each
      chat in the chat_group
    properties:
      id:
        type: string
        docs: >-
          Identifier for the Chat Group. Any Chat resumed from this Chat Group
          will have the same `chat_group_id`. Formatted as a UUID.
      first_start_timestamp:
        type: long
        docs: >-
          Time at which the first Chat in this Chat Group was created. Measured
          in seconds since the Unix epoch.
      most_recent_start_timestamp:
        type: long
        docs: >-
          Time at which the most recent Chat in this Chat Group was created.
          Measured in seconds since the Unix epoch.
      num_chats:
        type: integer
        docs: The total number of Chats associated with this Chat Group.
      page_number:
        type: integer
        docs: >-
          The page number of the returned list.


          This value corresponds to the `page_number` parameter specified in the
          request. Pagination uses zero-based indexing.
      page_size:
        type: integer
        docs: >-
          The maximum number of items returned per page.


          This value corresponds to the `page_size` parameter specified in the
          request.
      total_pages:
        type: integer
        docs: The total number of pages in the collection.
      pagination_direction:
        type: ReturnChatGroupPagedChatsPaginationDirection
        docs: >-
          Indicates the order in which the paginated results are presented,
          based on their creation date.


          It shows `ASC` for ascending order (chronological, with the oldest
          records first) or `DESC` for descending order (reverse-chronological,
          with the newest records first). This value corresponds to the
          `ascending_order` query parameter used in the request.
      chats_page:
        docs: List of Chats for the specified `page_number` and `page_size`.
        type: list<ReturnChat>
      active:
        type: optional<boolean>
        docs: >-
          Denotes whether there is an active Chat associated with this Chat
          Group.
    source:
      openapi: stenographer-openapi.json
  ReturnChatGroupPagedEventsPaginationDirection:
    enum:
      - ASC
      - DESC
    docs: >-
      Indicates the order in which the paginated results are presented, based on
      their creation date.


      It shows `ASC` for ascending order (chronological, with the oldest records
      first) or `DESC` for descending order (reverse-chronological, with the
      newest records first). This value corresponds to the `ascending_order`
      query parameter used in the request.
    source:
      openapi: stenographer-openapi.json
  ReturnChatGroupPagedEvents:
    docs: >-
      A paginated list of chat events that occurred across chats in this
      chat_group from the server
    properties:
      id:
        type: string
        docs: >-
          Identifier for the Chat Group. Any Chat resumed from this Chat Group
          will have the same `chat_group_id`. Formatted as a UUID.
      page_number:
        type: integer
        docs: >-
          The page number of the returned list.


          This value corresponds to the `page_number` parameter specified in the
          request. Pagination uses zero-based indexing.
      page_size:
        type: integer
        docs: >-
          The maximum number of items returned per page.


          This value corresponds to the `page_size` parameter specified in the
          request.
      total_pages:
        type: integer
        docs: The total number of pages in the collection.
      pagination_direction:
        type: ReturnChatGroupPagedEventsPaginationDirection
        docs: >-
          Indicates the order in which the paginated results are presented,
          based on their creation date.


          It shows `ASC` for ascending order (chronological, with the oldest
          records first) or `DESC` for descending order (reverse-chronological,
          with the newest records first). This value corresponds to the
          `ascending_order` query parameter used in the request.
      events_page:
        docs: List of Chat Events for the specified `page_number` and `page_size`.
        type: list<ReturnChatEvent>
    source:
      openapi: stenographer-openapi.json
  ReturnChatGroupPagedAudioReconstructionsPaginationDirection:
    enum:
      - ASC
      - DESC
    docs: >-
      Indicates the order in which the paginated results are presented, based on
      their creation date.


      It shows `ASC` for ascending order (chronological, with the oldest records
      first) or `DESC` for descending order (reverse-chronological, with the
      newest records first). This value corresponds to the `ascending_order`
      query parameter used in the request.
    source:
      openapi: stenographer-openapi.json
  ReturnChatGroupPagedAudioReconstructions:
    docs: A paginated list of chat reconstructions for a particular chatgroup
    properties:
      id:
        type: string
        docs: Identifier for the chat group. Formatted as a UUID.
      user_id:
        type: string
        docs: Identifier for the user that owns this chat. Formatted as a UUID.
      num_chats:
        type: integer
        docs: Total number of chats in this chatgroup
      page_number:
        type: integer
        docs: >-
          The page number of the returned list.


          This value corresponds to the `page_number` parameter specified in the
          request. Pagination uses zero-based indexing.
      page_size:
        type: integer
        docs: >-
          The maximum number of items returned per page.


          This value corresponds to the `page_size` parameter specified in the
          request.
      total_pages:
        type: integer
        docs: The total number of pages in the collection.
      pagination_direction:
        type: ReturnChatGroupPagedAudioReconstructionsPaginationDirection
        docs: >-
          Indicates the order in which the paginated results are presented,
          based on their creation date.


          It shows `ASC` for ascending order (chronological, with the oldest
          records first) or `DESC` for descending order (reverse-chronological,
          with the newest records first). This value corresponds to the
          `ascending_order` query parameter used in the request.
      audio_reconstructions_page:
        docs: >-
          List of chat audio reconstructions returned for the specified page
          number and page size.
        type: list<ReturnChatAudioReconstruction>
    source:
      openapi: stenographer-openapi.json
  PostedPromptSpec:
    docs: A Prompt associated with this Config.
    properties:
      version: optional<unknown>
    source:
      openapi: stenographer-openapi.json
  AssistantInput:
    docs: When provided, the input is spoken by EVI.
    properties:
      type:
        type: literal<"assistant_input">
        docs: >-
          The type of message sent through the socket; must be `assistant_input`
          for our server to correctly identify and process it as an Assistant
          Input message.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      text:
        type: string
        docs: >-
          Assistant text to synthesize into spoken audio and insert into the
          conversation.


          EVI uses this text to generate spoken audio using our proprietary
          expressive text-to-speech model. Our model adds appropriate emotional
          inflections and tones to the text based on the user’s expressions and
          the context of the conversation. The synthesized audio is streamed
          back to the user as an [Assistant
          Message](/reference/empathic-voice-interface-evi/chat/chat#receive.Assistant%20Message.type).
    source:
      openapi: assistant-asyncapi.json
  AudioConfiguration:
    properties:
      encoding:
        type: Encoding
        docs: Encoding format of the audio input, such as `linear16`.
      channels:
        type: integer
        docs: Number of audio channels.
      sample_rate:
        type: integer
        docs: >-
          Audio sample rate. Number of samples per second in the audio input,
          measured in Hertz.
    source:
      openapi: assistant-asyncapi.json
  AudioInput:
    docs: When provided, the input is audio.
    properties:
      type:
        type: literal<"audio_input">
        docs: >-
          The type of message sent through the socket; must be `audio_input` for
          our server to correctly identify and process it as an Audio Input
          message.


          This message is used for sending audio input data to EVI for
          processing and expression measurement. Audio data should be sent as a
          continuous stream, encoded in Base64.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      data:
        type: string
        docs: >-
          Base64 encoded audio input to insert into the conversation.


          The content of an Audio Input message is treated as the user’s speech
          to EVI and must be streamed continuously. Pre-recorded audio files are
          not supported.


          For optimal transcription quality, the audio data should be
          transmitted in small chunks.


          Hume recommends streaming audio with a buffer window of 20
          milliseconds (ms), or 100 milliseconds (ms) for web applications.
    source:
      openapi: assistant-asyncapi.json
  BuiltInTool:
    enum:
      - web_search
      - hang_up
    docs: >-
      Name of the built-in tool. Set to `web_search` to equip EVI with the
      built-in Web Search tool.
    source:
      openapi: assistant-asyncapi.json
  BuiltinToolConfig:
    properties:
      name:
        type: BuiltInTool
      fallback_content:
        type: optional<string>
        docs: >-
          Optional text passed to the supplemental LLM if the tool call fails.
          The LLM then uses this text to generate a response back to the user,
          ensuring continuity in the conversation.
    source:
      openapi: assistant-asyncapi.json
  Context:
    properties:
      type:
        type: optional<ContextType>
        docs: >-
          The persistence level of the injected context. Specifies how long the
          injected context will remain active in the session.


          There are three possible context types:


          - **Persistent**: The context is appended to all user messages for the
          duration of the session.


          - **Temporary**: The context is appended only to the next user
          message.

           - **Editable**: The original context is updated to reflect the new context.

           If the type is not specified, it will default to `temporary`.
      text:
        type: string
        docs: >-
          The context to be injected into the conversation. Helps inform the
          LLM's response by providing relevant information about the ongoing
          conversation.


          This text will be appended to the end of user messages based on the
          chosen persistence level. For example, if you want to remind EVI of
          its role as a helpful weather assistant, the context you insert will
          be appended to the end of user messages as `{Context: You are a
          helpful weather assistant}`.
    source:
      openapi: assistant-asyncapi.json
  ContextType:
    enum:
      - editable
      - persistent
      - temporary
    source:
      openapi: assistant-asyncapi.json
  Encoding:
    type: literal<"linear16">
  ErrorLevel:
    type: literal<"warn">
  PauseAssistantMessage:
    docs: >-
      Pause responses from EVI. Chat history is still saved and sent after
      resuming. 
    properties:
      type:
        type: literal<"pause_assistant_message">
        docs: >-
          The type of message sent through the socket; must be
          `pause_assistant_message` for our server to correctly identify and
          process it as a Pause Assistant message.


          Once this message is sent, EVI will not respond until a [Resume
          Assistant
          message](/reference/empathic-voice-interface-evi/chat/chat#send.Resume%20Assistant%20Message.type)
          is sent. When paused, EVI won’t respond, but transcriptions of your
          audio inputs will still be recorded.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
    source:
      openapi: assistant-asyncapi.json
  ResumeAssistantMessage:
    docs: >-
      Resume responses from EVI. Chat history sent while paused will now be
      sent. 
    properties:
      type:
        type: literal<"resume_assistant_message">
        docs: >-
          The type of message sent through the socket; must be
          `resume_assistant_message` for our server to correctly identify and
          process it as a Resume Assistant message.


          Upon resuming, if any audio input was sent during the pause, EVI will
          retain context from all messages sent but only respond to the last
          user message. (e.g., If you ask EVI two questions while paused and
          then send a `resume_assistant_message`, EVI will respond to the second
          question and have added the first question to its conversation
          context.)
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
    source:
      openapi: assistant-asyncapi.json
  SessionSettingsVariablesValue:
    discriminated: false
    union:
      - string
      - double
      - boolean
    source:
      openapi: assistant-asyncapi.json
    inline: true
  SessionSettings:
    docs: Settings for this chat session.
    properties:
      type:
        type: literal<"session_settings">
        docs: >-
          The type of message sent through the socket; must be
          `session_settings` for our server to correctly identify and process it
          as a Session Settings message.


          Session settings are temporary and apply only to the current Chat
          session. These settings can be adjusted dynamically based on the
          requirements of each session to ensure optimal performance and user
          experience.


          For more information, please refer to the [Session Settings
          section](/docs/empathic-voice-interface-evi/configuration#session-settings)
          on the EVI Configuration page.
      custom_session_id:
        type: optional<string>
        docs: >-
          Unique identifier for the session. Used to manage conversational
          state, correlate frontend and backend data, and persist conversations
          across EVI sessions.


          If included, the response sent from Hume to your backend will include
          this ID. This allows you to correlate frontend users with their
          incoming messages.


          It is recommended to pass a `custom_session_id` if you are using a
          Custom Language Model. Please see our guide to [using a custom
          language
          model](/docs/empathic-voice-interface-evi/custom-language-model) with
          EVI to learn more.
      system_prompt:
        type: optional<string>
        docs: >-
          Instructions used to shape EVI’s behavior, responses, and style for
          the session.


          When included in a Session Settings message, the provided Prompt
          overrides the existing one specified in the EVI configuration. If no
          Prompt was defined in the configuration, this Prompt will be the one
          used for the session.


          You can use the Prompt to define a specific goal or role for EVI,
          specifying how it should act or what it should focus on during the
          conversation. For example, EVI can be instructed to act as a customer
          support representative, a fitness coach, or a travel advisor, each
          with its own set of behaviors and response styles.


          For help writing a system prompt, see our [Prompting
          Guide](/docs/empathic-voice-interface-evi/prompting).
      context:
        type: optional<Context>
        docs: >-
          Allows developers to inject additional context into the conversation,
          which is appended to the end of user messages for the session.


          When included in a Session Settings message, the provided context can
          be used to remind the LLM of its role in every user message, prevent
          it from forgetting important details, or add new relevant information
          to the conversation.


          Set to `null` to disable context injection.
      audio:
        type: optional<AudioConfiguration>
        docs: >-
          Configuration details for the audio input used during the session.
          Ensures the audio is being correctly set up for processing.


          This optional field is only required when the audio input is encoded
          in PCM Linear 16 (16-bit, little-endian, signed PCM WAV data). For
          detailed instructions on how to configure session settings for PCM
          Linear 16 audio, please refer to the [Session Settings
          section](/docs/empathic-voice-interface-evi/configuration#session-settings)
          on the EVI Configuration page.
      language_model_api_key:
        type: optional<string>
        docs: >-
          Third party API key for the supplemental language model.


          When provided, EVI will use this key instead of Hume’s API key for the
          supplemental LLM. This allows you to bypass rate limits and utilize
          your own API key as needed.
      tools:
        type: optional<list<Tool>>
        docs: >-
          List of user-defined tools to enable for the session.


          Tools are resources used by EVI to perform various tasks, such as
          searching the web or calling external APIs. Built-in tools, like web
          search, are natively integrated, while user-defined tools are created
          and invoked by the user. To learn more, see our [Tool Use
          Guide](/docs/empathic-voice-interface-evi/tool-use).
      builtin_tools:
        type: optional<list<BuiltinToolConfig>>
        docs: >-
          List of built-in tools to enable for the session.


          Tools are resources used by EVI to perform various tasks, such as
          searching the web or calling external APIs. Built-in tools, like web
          search, are natively integrated, while user-defined tools are created
          and invoked by the user. To learn more, see our [Tool Use
          Guide](/docs/empathic-voice-interface-evi/tool-use).


          Currently, the only built-in tool Hume provides is **Web Search**.
          When enabled, Web Search equips EVI with the ability to search the web
          for up-to-date information.
      metadata:
        type: optional<map<string, unknown>>
      variables:
        type: optional<map<string, SessionSettingsVariablesValue>>
        docs: >-
          This field allows you to assign values to dynamic variables referenced
          in your system prompt.


          Each key represents the variable name, and the corresponding value is
          the specific content you wish to assign to that variable within the
          session. While the values for variables can be strings, numbers, or
          booleans, the value will ultimately be converted to a string when
          injected into your system prompt.


          Using this field, you can personalize responses based on
          session-specific details. For more guidance, see our [guide on using
          dynamic
          variables](/docs/empathic-voice-interface-evi/conversational-controls#dynamic-variables).
    source:
      openapi: assistant-asyncapi.json
  Tool:
    properties:
      type:
        type: ToolType
        docs: Type of tool. Set to `function` for user-defined tools.
      name:
        type: string
        docs: Name of the user-defined tool to be enabled.
      parameters:
        type: string
        docs: >-
          Parameters of the tool. Is a stringified JSON schema.


          These parameters define the inputs needed for the tool’s execution,
          including the expected data type and description for each input field.
          Structured as a JSON schema, this format ensures the tool receives
          data in the expected format.
      description:
        type: optional<string>
        docs: >-
          An optional description of what the tool does, used by the
          supplemental LLM to choose when and how to call the function.
      fallback_content:
        type: optional<string>
        docs: >-
          Optional text passed to the supplemental LLM if the tool call fails.
          The LLM then uses this text to generate a response back to the user,
          ensuring continuity in the conversation.
    source:
      openapi: assistant-asyncapi.json
  ToolErrorMessage:
    docs: When provided, the output is a function call error.
    properties:
      type:
        type: literal<"tool_error">
        docs: >-
          The type of message sent through the socket; for a Tool Error message,
          this must be `tool_error`.


          Upon receiving a [Tool Call
          message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type)
          and failing to invoke the function, this message is sent to notify EVI
          of the tool's failure.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      tool_type:
        type: optional<ToolType>
        docs: >-
          Type of tool called. Either `builtin` for natively implemented tools,
          like web search, or `function` for user-defined tools.
      tool_call_id:
        type: string
        docs: >-
          The unique identifier for a specific tool call instance.


          This ID is used to track the request and response of a particular tool
          invocation, ensuring that the Tool Error message is linked to the
          appropriate tool call request. The specified `tool_call_id` must match
          the one received in the [Tool Call
          message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type).
      content:
        type: optional<string>
        docs: >-
          Optional text passed to the supplemental LLM in place of the tool call
          result. The LLM then uses this text to generate a response back to the
          user, ensuring continuity in the conversation if the tool errors.
      error:
        type: string
        docs: Error message from the tool call, not exposed to the LLM or user.
      code:
        type: optional<string>
        docs: Error code. Identifies the type of error encountered.
      level:
        type: optional<ErrorLevel>
        docs: >-
          Indicates the severity of an error; for a Tool Error message, this
          must be `warn` to signal an unexpected event.
    source:
      openapi: assistant-asyncapi.json
  ToolResponseMessage:
    docs: When provided, the output is a function call response.
    properties:
      type:
        type: literal<"tool_response">
        docs: >-
          The type of message sent through the socket; for a Tool Response
          message, this must be `tool_response`.


          Upon receiving a [Tool Call
          message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type)
          and successfully invoking the function, this message is sent to convey
          the result of the function call back to EVI.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      tool_call_id:
        type: string
        docs: >-
          The unique identifier for a specific tool call instance.


          This ID is used to track the request and response of a particular tool
          invocation, ensuring that the correct response is linked to the
          appropriate request. The specified `tool_call_id` must match the one
          received in the [Tool Call
          message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.tool_call_id).
      content:
        type: string
        docs: >-
          Return value of the tool call. Contains the output generated by the
          tool to pass back to EVI.
      tool_name:
        type: optional<string>
        docs: >-
          Name of the tool.


          Include this optional field to help the supplemental LLM identify
          which tool generated the response. The specified `tool_name` must
          match the one received in the [Tool Call
          message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type).
      tool_type:
        type: optional<ToolType>
        docs: >-
          Type of tool called. Either `builtin` for natively implemented tools,
          like web search, or `function` for user-defined tools.
    source:
      openapi: assistant-asyncapi.json
  ToolType:
    enum:
      - builtin
      - function
    source:
      openapi: assistant-asyncapi.json
  UserInput:
    docs: >-
      User text to insert into the conversation. Text sent through a User Input
      message is treated as the user’s speech to EVI. EVI processes this input
      and provides a corresponding response.


      Expression measurement results are not available for User Input messages,
      as the prosody model relies on audio input and cannot process text alone.
    properties:
      type:
        type: literal<"user_input">
        docs: >-
          The type of message sent through the socket; must be `user_input` for
          our server to correctly identify and process it as a User Input
          message.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      text:
        type: string
        docs: >-
          User text to insert into the conversation. Text sent through a User
          Input message is treated as the user’s speech to EVI. EVI processes
          this input and provides a corresponding response.


          Expression measurement results are not available for User Input
          messages, as the prosody model relies on audio input and cannot
          process text alone.
    source:
      openapi: assistant-asyncapi.json
  AssistantEnd:
    docs: When provided, the output is an assistant end message.
    properties:
      type:
        type: literal<"assistant_end">
        docs: >-
          The type of message sent through the socket; for an Assistant End
          message, this must be `assistant_end`.


          This message indicates the conclusion of the assistant’s response,
          signaling that the assistant has finished speaking for the current
          conversational turn.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
    source:
      openapi: assistant-asyncapi.json
  AssistantMessage:
    docs: When provided, the output is an assistant message.
    properties:
      type:
        type: literal<"assistant_message">
        docs: >-
          The type of message sent through the socket; for an Assistant Message,
          this must be `assistant_message`.


          This message contains both a transcript of the assistant’s response
          and the expression measurement predictions of the assistant’s audio
          output.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      id:
        type: optional<string>
        docs: >-
          ID of the assistant message. Allows the Assistant Message to be
          tracked and referenced.
      message:
        type: ChatMessage
        docs: Transcript of the message.
      models:
        type: Inference
        docs: Inference model results.
      from_text:
        type: boolean
        docs: >-
          Indicates if this message was inserted into the conversation as text
          from an [Assistant Input
          message](/reference/empathic-voice-interface-evi/chat/chat#send.Assistant%20Input.text).
    source:
      openapi: assistant-asyncapi.json
  AudioOutput:
    docs: >-
      The type of message sent through the socket; for an Audio Output message,
      this must be `audio_output`.
    properties:
      type:
        type: literal<"audio_output">
        docs: >-
          The type of message sent through the socket; for an Audio Output
          message, this must be `audio_output`.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      id:
        type: string
        docs: >-
          ID of the audio output. Allows the Audio Output message to be tracked
          and referenced.
      index:
        type: integer
        docs: Index of the chunk of audio relative to the whole audio segment.
      data:
        type: string
        docs: >-
          Base64 encoded audio output. This encoded audio is transmitted to the
          client, where it can be decoded and played back as part of the user
          interaction.
    source:
      openapi: assistant-asyncapi.json
  ChatMessageToolResult:
    discriminated: false
    docs: Function call response from client.
    union:
      - type: ToolResponseMessage
      - type: ToolErrorMessage
    source:
      openapi: assistant-asyncapi.json
    inline: true
  ChatMessage:
    properties:
      role:
        type: Role
        docs: Role of who is providing the message.
      content:
        type: optional<string>
        docs: Transcript of the message.
      tool_call:
        type: optional<ToolCallMessage>
        docs: Function call name and arguments.
      tool_result:
        type: optional<ChatMessageToolResult>
        docs: Function call response from client.
    source:
      openapi: assistant-asyncapi.json
  ChatMetadata:
    docs: When provided, the output is a chat metadata message.
    properties:
      type:
        type: literal<"chat_metadata">
        docs: >-
          The type of message sent through the socket; for a Chat Metadata
          message, this must be `chat_metadata`.


          The Chat Metadata message is the first message you receive after
          establishing a connection with EVI and contains important identifiers
          for the current Chat session.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      chat_group_id:
        type: string
        docs: >-
          ID of the Chat Group.


          Used to resume a Chat when passed in the
          [resumed_chat_group_id](/reference/empathic-voice-interface-evi/chat/chat#request.query.resumed_chat_group_id)
          query parameter of a subsequent connection request. This allows EVI to
          continue the conversation from where it left off within the Chat
          Group.


          Learn more about [supporting chat
          resumability](/docs/empathic-voice-interface-evi/faq#does-evi-support-chat-resumability)
          from the EVI FAQ.
      chat_id:
        type: string
        docs: >-
          ID of the Chat session. Allows the Chat session to be tracked and
          referenced.
      request_id:
        type: optional<string>
        docs: ID of the initiating request.
    source:
      openapi: assistant-asyncapi.json
  EmotionScores:
    properties:
      Admiration: double
      Adoration: double
      Aesthetic Appreciation: double
      Amusement: double
      Anger: double
      Anxiety: double
      Awe: double
      Awkwardness: double
      Boredom: double
      Calmness: double
      Concentration: double
      Confusion: double
      Contemplation: double
      Contempt: double
      Contentment: double
      Craving: double
      Desire: double
      Determination: double
      Disappointment: double
      Disgust: double
      Distress: double
      Doubt: double
      Ecstasy: double
      Embarrassment: double
      Empathic Pain: double
      Entrancement: double
      Envy: double
      Excitement: double
      Fear: double
      Guilt: double
      Horror: double
      Interest: double
      Joy: double
      Love: double
      Nostalgia: double
      Pain: double
      Pride: double
      Realization: double
      Relief: double
      Romance: double
      Sadness: double
      Satisfaction: double
      Shame: double
      Surprise (negative): double
      Surprise (positive): double
      Sympathy: double
      Tiredness: double
      Triumph: double
    source:
      openapi: assistant-asyncapi.json
  WebSocketError:
    docs: When provided, the output is an error message.
    properties:
      type:
        type: literal<"error">
        docs: >-
          The type of message sent through the socket; for a Web Socket Error
          message, this must be `error`.


          This message indicates a disruption in the WebSocket connection, such
          as an unexpected disconnection, protocol error, or data transmission
          issue.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      code:
        type: string
        docs: Error code. Identifies the type of error encountered.
      slug:
        type: string
        docs: >-
          Short, human-readable identifier and description for the error. See a
          complete list of error slugs on the [Errors
          page](/docs/resources/errors).
      message:
        type: string
        docs: Detailed description of the error.
    source:
      openapi: assistant-asyncapi.json
  Inference:
    properties:
      prosody:
        type: optional<ProsodyInference>
        docs: >-
          Prosody model inference results.


          EVI uses the prosody model to measure 48 emotions related to speech
          and vocal characteristics within a given expression.
    source:
      openapi: assistant-asyncapi.json
  MillisecondInterval:
    properties:
      begin:
        type: integer
        docs: Start time of the interval in milliseconds.
      end:
        type: integer
        docs: End time of the interval in milliseconds.
    source:
      openapi: assistant-asyncapi.json
  ProsodyInference:
    properties:
      scores:
        type: EmotionScores
        docs: >-
          The confidence scores for 48 emotions within the detected expression
          of an audio sample.


          Scores typically range from 0 to 1, with higher values indicating a
          stronger confidence level in the measured attribute.


          See our guide on [interpreting expression measurement
          results](/docs/expression-measurement/faq#how-do-i-interpret-my-results)
          to learn more.
    source:
      openapi: assistant-asyncapi.json
  Role:
    enum:
      - assistant
      - system
      - user
      - all
      - tool
    source:
      openapi: assistant-asyncapi.json
  ToolCallMessage:
    docs: When provided, the output is a tool call.
    properties:
      name:
        type: string
        docs: Name of the tool called.
      parameters:
        type: string
        docs: >-
          Parameters of the tool.


          These parameters define the inputs needed for the tool’s execution,
          including the expected data type and description for each input field.
          Structured as a stringified JSON schema, this format ensures the tool
          receives data in the expected format.
      tool_call_id:
        type: string
        docs: >-
          The unique identifier for a specific tool call instance.


          This ID is used to track the request and response of a particular tool
          invocation, ensuring that the correct response is linked to the
          appropriate request.
      type:
        type: literal<"tool_call">
        docs: >-
          The type of message sent through the socket; for a Tool Call message,
          this must be `tool_call`.


          This message indicates that the supplemental LLM has detected a need
          to invoke the specified tool.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      tool_type:
        type: optional<ToolType>
        docs: >-
          Type of tool called. Either `builtin` for natively implemented tools,
          like web search, or `function` for user-defined tools.
      response_required:
        type: boolean
        docs: >-
          Indicates whether a response to the tool call is required from the
          developer, either in the form of a [Tool Response
          message](/reference/empathic-voice-interface-evi/chat/chat#send.Tool%20Response%20Message.type)
          or a [Tool Error
          message](/reference/empathic-voice-interface-evi/chat/chat#send.Tool%20Error%20Message.type).
    source:
      openapi: assistant-asyncapi.json
  UserInterruption:
    docs: When provided, the output is an interruption.
    properties:
      type:
        type: literal<"user_interruption">
        docs: >-
          The type of message sent through the socket; for a User Interruption
          message, this must be `user_interruption`.


          This message indicates the user has interrupted the assistant’s
          response. EVI detects the interruption in real-time and sends this
          message to signal the interruption event. This message allows the
          system to stop the current audio playback, clear the audio queue, and
          prepare to handle new user input.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      time:
        type: integer
        docs: Unix timestamp of the detected user interruption.
    source:
      openapi: assistant-asyncapi.json
  UserMessage:
    docs: When provided, the output is a user message.
    properties:
      type:
        type: literal<"user_message">
        docs: >-
          The type of message sent through the socket; for a User Message, this
          must be `user_message`.


          This message contains both a transcript of the user’s input and the
          expression measurement predictions if the input was sent as an [Audio
          Input
          message](/reference/empathic-voice-interface-evi/chat/chat#send.Audio%20Input.type).
          Expression measurement predictions are not provided for a [User Input
          message](/reference/empathic-voice-interface-evi/chat/chat#send.User%20Input.type),
          as the prosody model relies on audio input and cannot process text
          alone.
      custom_session_id:
        type: optional<string>
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
      message:
        type: ChatMessage
        docs: Transcript of the message.
      models:
        type: Inference
        docs: Inference model results.
      time:
        type: MillisecondInterval
        docs: Start and End time of user message.
      from_text:
        type: boolean
        docs: >-
          Indicates if this message was inserted into the conversation as text
          from a [User
          Input](/reference/empathic-voice-interface-evi/chat/chat#send.User%20Input.text)
          message.
      interim:
        type: boolean
        docs: >-
          Indicates if this message contains an immediate and unfinalized
          transcript of the user’s audio input. If it does, words may be
          repeated across successive `UserMessage` messages as our transcription
          model becomes more confident about what was said with additional
          context. Interim messages are useful to detect if the user is
          interrupting during audio playback on the client. Even without a
          finalized transcription, along with
          [UserInterrupt](/reference/empathic-voice-interface-evi/chat/chat#receive.User%20Interruption.type)
          messages, interim `UserMessages` are useful for detecting if the user
          is interrupting during audio playback on the client, signaling to stop
          playback in your application. Interim `UserMessages` will only be
          received if the
          [verbose_transcription](/reference/empathic-voice-interface-evi/chat/chat#request.query.verbose_transcription)
          query parameter is set to `true` in the handshake request.
    source:
      openapi: assistant-asyncapi.json
  JsonMessage:
    discriminated: false
    union:
      - type: AssistantEnd
      - type: AssistantMessage
      - type: ChatMetadata
      - type: WebSocketError
      - type: UserInterruption
      - type: UserMessage
      - type: ToolCallMessage
      - type: ToolResponseMessage
      - type: ToolErrorMessage
    source:
      openapi: assistant-asyncapi.json
  TtsInput:
    properties:
      type: optional<literal<"tts">>
    source:
      openapi: assistant-asyncapi.json
  TextInput:
    properties:
      type: optional<literal<"text_input">>
    source:
      openapi: assistant-asyncapi.json
  FunctionCallResponseInput:
    properties:
      type: optional<literal<"function_call_response">>
    source:
      openapi: assistant-asyncapi.json
  HTTPValidationError:
    properties:
      detail:
        type: optional<list<ValidationError>>
    source:
      openapi: assistant-openapi.json
  ValidationErrorLocItem:
    discriminated: false
    union:
      - string
      - integer
    source:
      openapi: assistant-openapi.json
    inline: true
  ValidationError:
    properties:
      loc:
        type: list<ValidationErrorLocItem>
      msg: string
      type: string
    source:
      openapi: assistant-openapi.json
  VoiceNameEnum:
    enum:
      - ITO
      - KORA
      - DACHER
      - AURA
      - FINN
      - SIENNA
      - WILLOW
      - SCOUT
      - WHIMSY
      - ACE
      - JUNO
      - STELLA
      - HIRO
      - SUNNY
    source:
      openapi: assistant-openapi.json
