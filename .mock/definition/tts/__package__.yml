errors:
  UnprocessableEntityError:
    status-code: 422
    type: HTTPValidationError
    docs: Validation Error
    examples:
      - value: {}
  BadRequestError:
    status-code: 400
    type: ErrorResponse
    docs: Bad Request
    examples:
      - value: {}
service:
  auth: false
  base-path: ''
  endpoints:
    synthesize-json:
      path: /v0/tts
      method: POST
      docs: >-
        Synthesizes one or more input texts into speech using the specified
        voice. If no voice is provided, a novel voice will be generated
        dynamically. Optionally, additional context can be included to influence
        the speech's style and prosody.


        The response includes the base64-encoded audio and metadata in JSON
        format.
      source:
        openapi: tts-openapi.json
      display-name: Text-to-Speech (Json)
      request:
        body:
          type: PostedTts
        content-type: application/json
      response:
        docs: Successful Response
        type: ReturnTts
        status-code: 200
      errors:
        - UnprocessableEntityError
      examples:
        - request:
            context:
              utterances:
                - text: How can people see beauty so differently?
                  description: >-
                    A curious student with a clear and respectful tone, seeking
                    clarification on Hume's ideas with a straightforward
                    question.
            format:
              type: mp3
            num_generations: 1
            utterances:
              - text: >-
                  Beauty is no quality in things themselves: It exists merely in
                  the mind which contemplates them.
                description: >-
                  Middle-aged masculine voice with a clear, rhythmic Scots lilt,
                  rounded vowels, and a warm, steady tone with an articulate,
                  academic quality.
          response:
            body:
              generations:
                - audio: //PExAA0DDYRvkpNfhv3JI5JZ...etc.
                  duration: 7.44225
                  encoding:
                    format: mp3
                    sample_rate: 48000
                  file_size: 120192
                  generation_id: 795c949a-1510-4a80-9646-7d0863b023ab
                  snippets:
                    - - audio: //PExAA0DDYRvkpNfhv3JI5JZ...etc.
                        generation_id: 795c949a-1510-4a80-9646-7d0863b023ab
                        id: 37b1b1b1-1b1b-1b1b-1b1b-1b1b1b1b1b1b
                        text: >-
                          Beauty is no quality in things themselves: It exists
                          merely in the mind which contemplates them.
                        utterance_index: 0
                        timestamps: []
              request_id: 66e01f90-4501-4aa0-bbaf-74f45dc15aa725906
    synthesize-file:
      path: /v0/tts/file
      method: POST
      docs: >-
        Synthesizes one or more input texts into speech using the specified
        voice. If no voice is provided, a novel voice will be generated
        dynamically. Optionally, additional context can be included to influence
        the speech's style and prosody. 


        The response contains the generated audio file in the requested format.
      source:
        openapi: tts-openapi.json
      display-name: Text-to-Speech (File)
      request:
        body:
          type: PostedTts
        content-type: application/json
      response:
        docs: OK
        type: file
        status-code: 200
      errors:
        - UnprocessableEntityError
      examples:
        - request:
            context:
              generation_id: 09ad914d-8e7f-40f8-a279-e34f07f7dab2
            format:
              type: mp3
            num_generations: 1
            utterances:
              - text: >-
                  Beauty is no quality in things themselves: It exists merely in
                  the mind which contemplates them.
                description: >-
                  Middle-aged masculine voice with a clear, rhythmic Scots lilt,
                  rounded vowels, and a warm, steady tone with an articulate,
                  academic quality.
    synthesize-json-streaming:
      path: /v0/tts/stream/json
      method: POST
      docs: >-
        Streams synthesized speech using the specified voice. If no voice is
        provided, a novel voice will be generated dynamically. Optionally,
        additional context can be included to influence the speech's style and
        prosody. 


        The response is a stream of JSON objects including audio encoded in
        base64.
      source:
        openapi: tts-openapi.json
      display-name: Text-to-Speech (Streamed JSON)
      request:
        body:
          type: PostedTts
        content-type: application/json
      response-stream:
        docs: Successful Response
        type: TtsOutput
        format: json
      errors:
        - UnprocessableEntityError
      examples:
        - request:
            utterances:
              - text: >-
                  Beauty is no quality in things themselves: It exists merely in
                  the mind which contemplates them.
                voice:
                  name: Male English Actor
                  provider: HUME_AI
    voice_conversion_stream_json_v0_tts_voice_conversion_json_post:
      path: /v0/tts/voice_conversion/json
      method: POST
      source:
        openapi: tts-openapi.json
      display-name: Voice Conversion Stream Json
      request:
        name: Body_voice_conversion_stream_json_v0_tts_voice_conversion_json_post
        query-parameters:
          access_token:
            type: optional<string>
            default: ''
            docs: >-
              Access token used for authenticating the client. If not provided,
              an `api_key` must be provided to authenticate.


              The access token is generated using both an API key and a Secret
              key, which provides an additional layer of security compared to
              using just an API key.


              For more details, refer to the [Authentication Strategies
              Guide](/docs/introduction/api-key#authentication-strategies).
        body:
          properties:
            audio: file
        content-type: multipart/form-data
      response:
        docs: Successful Response
        type: VoiceConversionStreamJsonV0TtsVoiceConversionJsonPostResponse
        status-code: 200
      errors:
        - UnprocessableEntityError
      examples:
        - query-parameters:
            access_token: access_token
          request: {}
          response:
            body:
              type: timestamp
              request_id: request_id
              generation_id: generation_id
              snippet_id: snippet_id
              timestamp:
                type: word
                text: text
                time:
                  begin: 1
                  end: 1
    tts_conversion_stream_json_v0_tts_tts_conversion_json_post:
      path: /v0/tts/tts_conversion/json
      method: POST
      source:
        openapi: tts-openapi.json
      display-name: Tts Conversion Stream Json
      request:
        name: TtsConversionStreamJsonV0TtsTtsConversionJsonPostRequest
        query-parameters:
          access_token:
            type: optional<string>
            default: ''
            docs: >-
              Access token used for authenticating the client. If not provided,
              an `api_key` must be provided to authenticate.


              The access token is generated using both an API key and a Secret
              key, which provides an additional layer of security compared to
              using just an API key.


              For more details, refer to the [Authentication Strategies
              Guide](/docs/introduction/api-key#authentication-strategies).
        body:
          properties:
            model:
              type: optional<literal<"octave">>
              docs: The TTS model to use for speech generations.
            version: optional<unknown>
            context[generation_id]:
              type: optional<string>
              docs: >-
                The ID of a prior TTS generation to use as context for
                generating consistent speech style and prosody across multiple
                requests. Including context may increase audio generation times.
            context[utterances][<n>][text]:
              type: optional<string>
              docs: The input text to be converted to speech output.
              validation:
                maxLength: 5000
            context[utterances][<n>][description]:
              type: optional<string>
              docs: >-
                Natural language instructions describing how the text should be
                spoken by the model (e.g., `"a soft, gentle voice with a strong
                British accent"`).
              validation:
                maxLength: 1000
            context[utterances][<n>][voice][id]:
              type: optional<string>
              docs: ID of the voice in the `Voice Library`.
            context[utterances][<n>][voice][provider]: optional<unknown>
            context[utterances][<n>][voice][name]:
              type: optional<string>
              docs: Name of the voice in the `Voice Library`.
            context[utterances][<n>][speed]:
              type: optional<double>
              docs: A relative measure of how fast this utterance should be spoken.
              validation:
                min: 0.25
                max: 3
            context[utterances][<n>][trailing_silence]:
              type: optional<double>
              docs: >-
                Duration of trailing silence (in seconds) to add to this
                utterance
              validation:
                min: 0
                max: 5
            utterances[<n>][text]:
              type: optional<string>
              docs: The input text to be converted to speech output.
              validation:
                maxLength: 5000
            utterances[<n>][description]:
              type: optional<string>
              docs: >-
                Natural language instructions describing how the text should be
                spoken by the model (e.g., `"a soft, gentle voice with a strong
                British accent"`).
              validation:
                maxLength: 1000
            utterances[<n>][voice][id]:
              type: optional<string>
              docs: ID of the voice in the `Voice Library`.
            utterances[<n>][voice][provider]: optional<unknown>
            utterances[<n>][voice][name]:
              type: optional<string>
              docs: Name of the voice in the `Voice Library`.
            utterances[<n>][speed]:
              type: optional<double>
              docs: A relative measure of how fast this utterance should be spoken.
              validation:
                min: 0.25
                max: 3
            utterances[<n>][trailing_silence]:
              type: optional<double>
              docs: >-
                Duration of trailing silence (in seconds) to add to this
                utterance
              validation:
                min: 0
                max: 5
            num_generations:
              type: optional<integer>
              docs: Number of generations of the audio to produce.
              validation:
                min: 1
                max: 5
            format[type]:
              type: optional<literal<"mp3">>
              docs: Format for the output audio.
            expand_description:
              type: optional<boolean>
              docs: >-
                If enabled, enhances the provided description prompt to improve
                voice generation quality.
            split_utterances:
              type: optional<boolean>
              docs: >-
                If enabled, each input utterance will be split as needed into
                more natural-sounding `snippets` of speech for audio generation.
            filter_generations:
              type: optional<boolean>
              docs: >-
                If enabled, additional generations will be made, and the best
                `num_generations` of them all will be returned.
            multi_speaker:
              type: optional<boolean>
              docs: >-
                If enabled, consecutive utterances with the different voices
                will be generated with compounding context that takes into
                account the previous utterances.
            strip_headers:
              type: optional<boolean>
              docs: >-
                If enabled, the audio for all the chunks of a generation, once
                concatenated together, will constitute a single audio file.
                Otherwise, if disabled, each chunk's audio will be its own audio
                file, each with its own headers (if applicable).
            include_timestamp_types[<n>]: optional<unknown>
            no_binary:
              type: optional<boolean>
              docs: >-
                If enabled, no binary websocket messages will be sent to the
                client.
            instant_mode:
              type: optional<boolean>
              docs: >-
                Accelerates processing to reduce streaming latency. Incurs
                approximately 10% additional cost while preserving full voice
                quality.
        content-type: multipart/form-data
      response:
        docs: Successful Response
        type: TtsConversionStreamJsonV0TtsTtsConversionJsonPostResponse
        status-code: 200
      errors:
        - UnprocessableEntityError
      examples:
        - query-parameters:
            access_token: access_token
          request: {}
          response:
            body:
              type: timestamp
              request_id: request_id
              generation_id: generation_id
              snippet_id: snippet_id
              timestamp:
                type: word
                text: text
                time:
                  begin: 1
                  end: 1
    synthesize-file-streaming:
      path: /v0/tts/stream/file
      method: POST
      docs: >-
        Streams synthesized speech using the specified voice. If no voice is
        provided, a novel voice will be generated dynamically. Optionally,
        additional context can be included to influence the speech's style and
        prosody.
      source:
        openapi: tts-openapi.json
      display-name: Text-to-Speech (Streamed File)
      request:
        body:
          type: PostedTts
        content-type: application/json
      response:
        docs: OK
        type: file
        status-code: 200
      errors:
        - UnprocessableEntityError
      examples:
        - request:
            utterances:
              - text: >-
                  Beauty is no quality in things themselves: It exists merely in
                  the mind which contemplates them.
                voice:
                  name: Male English Actor
                  provider: HUME_AI
    voice_conversion_stream_file_v0_tts_voice_conversion_file_post:
      path: /v0/tts/voice_conversion/file
      method: POST
      source:
        openapi: tts-openapi.json
      display-name: Voice Conversion Stream File
      request:
        name: Body_voice_conversion_stream_file_v0_tts_voice_conversion_file_post
        query-parameters:
          access_token:
            type: optional<string>
            default: ''
            docs: >-
              Access token used for authenticating the client. If not provided,
              an `api_key` must be provided to authenticate.


              The access token is generated using both an API key and a Secret
              key, which provides an additional layer of security compared to
              using just an API key.


              For more details, refer to the [Authentication Strategies
              Guide](/docs/introduction/api-key#authentication-strategies).
        body:
          properties:
            audio: file
        content-type: multipart/form-data
      response:
        docs: Successful Response
        type: unknown
        status-code: 200
      errors:
        - UnprocessableEntityError
      examples:
        - query-parameters:
            access_token: access_token
          request: {}
          response:
            body:
              key: value
    tts_conversion_stream_file_v0_tts_tts_conversion_file_post:
      path: /v0/tts/tts_conversion/file
      method: POST
      source:
        openapi: tts-openapi.json
      display-name: Tts Conversion Stream File
      request:
        name: TtsConversionStreamFileV0TtsTtsConversionFilePostRequest
        query-parameters:
          access_token:
            type: optional<string>
            default: ''
            docs: >-
              Access token used for authenticating the client. If not provided,
              an `api_key` must be provided to authenticate.


              The access token is generated using both an API key and a Secret
              key, which provides an additional layer of security compared to
              using just an API key.


              For more details, refer to the [Authentication Strategies
              Guide](/docs/introduction/api-key#authentication-strategies).
        body:
          properties:
            model:
              type: optional<literal<"octave">>
              docs: The TTS model to use for speech generations.
            version: optional<unknown>
            context[generation_id]:
              type: optional<string>
              docs: >-
                The ID of a prior TTS generation to use as context for
                generating consistent speech style and prosody across multiple
                requests. Including context may increase audio generation times.
            context[utterances][<n>][text]:
              type: optional<string>
              docs: The input text to be converted to speech output.
              validation:
                maxLength: 5000
            context[utterances][<n>][description]:
              type: optional<string>
              docs: >-
                Natural language instructions describing how the text should be
                spoken by the model (e.g., `"a soft, gentle voice with a strong
                British accent"`).
              validation:
                maxLength: 1000
            context[utterances][<n>][voice][id]:
              type: optional<string>
              docs: ID of the voice in the `Voice Library`.
            context[utterances][<n>][voice][provider]: optional<unknown>
            context[utterances][<n>][voice][name]:
              type: optional<string>
              docs: Name of the voice in the `Voice Library`.
            context[utterances][<n>][speed]:
              type: optional<double>
              docs: A relative measure of how fast this utterance should be spoken.
              validation:
                min: 0.25
                max: 3
            context[utterances][<n>][trailing_silence]:
              type: optional<double>
              docs: >-
                Duration of trailing silence (in seconds) to add to this
                utterance
              validation:
                min: 0
                max: 5
            utterances[<n>][text]:
              type: optional<string>
              docs: The input text to be converted to speech output.
              validation:
                maxLength: 5000
            utterances[<n>][description]:
              type: optional<string>
              docs: >-
                Natural language instructions describing how the text should be
                spoken by the model (e.g., `"a soft, gentle voice with a strong
                British accent"`).
              validation:
                maxLength: 1000
            utterances[<n>][voice][id]:
              type: optional<string>
              docs: ID of the voice in the `Voice Library`.
            utterances[<n>][voice][provider]: optional<unknown>
            utterances[<n>][voice][name]:
              type: optional<string>
              docs: Name of the voice in the `Voice Library`.
            utterances[<n>][speed]:
              type: optional<double>
              docs: A relative measure of how fast this utterance should be spoken.
              validation:
                min: 0.25
                max: 3
            utterances[<n>][trailing_silence]:
              type: optional<double>
              docs: >-
                Duration of trailing silence (in seconds) to add to this
                utterance
              validation:
                min: 0
                max: 5
            num_generations:
              type: optional<integer>
              docs: Number of generations of the audio to produce.
              validation:
                min: 1
                max: 5
            format[type]:
              type: optional<literal<"mp3">>
              docs: Format for the output audio.
            expand_description:
              type: optional<boolean>
              docs: >-
                If enabled, enhances the provided description prompt to improve
                voice generation quality.
            split_utterances:
              type: optional<boolean>
              docs: >-
                If enabled, each input utterance will be split as needed into
                more natural-sounding `snippets` of speech for audio generation.
            filter_generations:
              type: optional<boolean>
              docs: >-
                If enabled, additional generations will be made, and the best
                `num_generations` of them all will be returned.
            multi_speaker:
              type: optional<boolean>
              docs: >-
                If enabled, consecutive utterances with the different voices
                will be generated with compounding context that takes into
                account the previous utterances.
            strip_headers:
              type: optional<boolean>
              docs: >-
                If enabled, the audio for all the chunks of a generation, once
                concatenated together, will constitute a single audio file.
                Otherwise, if disabled, each chunk's audio will be its own audio
                file, each with its own headers (if applicable).
            include_timestamp_types[<n>]: optional<unknown>
            no_binary:
              type: optional<boolean>
              docs: >-
                If enabled, no binary websocket messages will be sent to the
                client.
            instant_mode:
              type: optional<boolean>
              docs: >-
                Accelerates processing to reduce streaming latency. Incurs
                approximately 10% additional cost while preserving full voice
                quality.
        content-type: multipart/form-data
      response:
        docs: Successful Response
        type: unknown
        status-code: 200
      errors:
        - UnprocessableEntityError
      examples:
        - query-parameters:
            access_token: access_token
          request: {}
          response:
            body:
              key: value
  source:
    openapi: tts-openapi.json
types:
  VoiceConversionStreamJsonV0TtsVoiceConversionJsonPostResponse:
    discriminated: false
    union:
      - type: TimestampMessage
      - type: SnippetAudioChunk
    source:
      openapi: tts-openapi.json
  TtsConversionStreamJsonV0TtsTtsConversionJsonPostResponse:
    discriminated: false
    union:
      - type: TimestampMessage
      - type: SnippetAudioChunk
    source:
      openapi: tts-openapi.json
  PublishTts:
    docs: Input message type for the TTS stream.
    properties:
      text:
        type: optional<string>
        docs: The input text to be converted to speech output.
        default: ''
        validation:
          maxLength: 5000
      description:
        type: optional<string>
        docs: >-
          Natural language instructions describing how the text should be spoken
          by the model (e.g., `"a soft, gentle voice with a strong British
          accent"`).
        validation:
          maxLength: 1000
      voice:
        type: optional<PostedUtteranceVoice>
        docs: >-
          The name or ID of the voice from the `Voice Library` to be used as the
          speaker for this and all subsequent utterances, until the `"voice"`
          field is updated again.
      speed:
        type: optional<double>
        docs: A relative measure of how fast this utterance should be spoken.
        default: 1
        validation:
          min: 0.25
          max: 3
      trailing_silence:
        type: optional<double>
        docs: Duration of trailing silence (in seconds) to add to this utterance
        default: 0
        validation:
          min: 0
          max: 5
      flush:
        type: optional<boolean>
        docs: >-
          Force the generation of audio regardless of how much text has been
          supplied.
        default: false
      close:
        type: optional<boolean>
        docs: Force the generation of audio and close the stream.
        default: false
    source:
      openapi: tts-asyncapi.json
  PostedUtteranceVoiceWithId:
    properties:
      id:
        type: string
        docs: The unique ID associated with the **Voice**.
      provider:
        type: optional<VoiceProvider>
        docs: >-
          Specifies the source provider associated with the chosen voice.


          - **`HUME_AI`**: Select voices from Hume's [Voice
          Library](https://platform.hume.ai/tts/voice-library), containing a
          variety of preset, shared voices.

          - **`CUSTOM_VOICE`**: Select from voices you've personally generated
          and saved in your account. 


          If no provider is explicitly set, the default provider is
          `CUSTOM_VOICE`. When using voices from Hume's **Voice Library**, you
          must explicitly set the provider to `HUME_AI`.


          Preset voices from Hume's **Voice Library** are accessible by all
          users. In contrast, your custom voices are private and accessible only
          via requests authenticated with your API key.
    source:
      openapi: tts-openapi.json
  PostedUtteranceVoiceWithName:
    properties:
      name:
        type: string
        docs: The name of a **Voice**.
      provider:
        type: optional<VoiceProvider>
        docs: >-
          Specifies the source provider associated with the chosen voice.


          - **`HUME_AI`**: Select voices from Hume's [Voice
          Library](https://platform.hume.ai/tts/voice-library), containing a
          variety of preset, shared voices.

          - **`CUSTOM_VOICE`**: Select from voices you've personally generated
          and saved in your account. 


          If no provider is explicitly set, the default provider is
          `CUSTOM_VOICE`. When using voices from Hume's **Voice Library**, you
          must explicitly set the provider to `HUME_AI`.


          Preset voices from Hume's **Voice Library** are accessible by all
          users. In contrast, your custom voices are private and accessible only
          via requests authenticated with your API key.
    source:
      openapi: tts-openapi.json
  VoiceProvider:
    enum:
      - HUME_AI
      - CUSTOM_VOICE
    source:
      openapi: tts-openapi.json
  PostedUtteranceVoice:
    discriminated: false
    union:
      - type: PostedUtteranceVoiceWithId
      - type: PostedUtteranceVoiceWithName
    source:
      openapi: tts-openapi.json
  AudioFormatType:
    enum:
      - mp3
      - pcm
      - wav
    source:
      openapi: tts-openapi.json
  MillisecondInterval:
    properties:
      begin:
        type: integer
        docs: Start time of the interval in milliseconds.
      end:
        type: integer
        docs: End time of the interval in milliseconds.
    source:
      openapi: tts-openapi.json
  TimestampMessage:
    docs: A word or phoneme level timestamp for the generated audio.
    properties:
      type: literal<"timestamp">
      request_id:
        type: string
        docs: ID of the initiating request.
      generation_id:
        type: string
        docs: >-
          The generation ID of the parent snippet that this chunk corresponds
          to.
      snippet_id:
        type: string
        docs: The ID of the parent snippet that this chunk corresponds to.
      timestamp:
        type: Timestamp
        docs: A word or phoneme level timestamp for the generated audio.
    source:
      openapi: tts-openapi.json
  SnippetAudioChunk:
    docs: Metadata for a chunk of generated audio.
    properties:
      type: literal<"audio">
      request_id:
        type: string
        docs: ID of the initiating request.
      generation_id:
        type: string
        docs: >-
          The generation ID of the parent snippet that this chunk corresponds
          to.
      snippet_id:
        type: string
        docs: The ID of the parent snippet that this chunk corresponds to.
      text:
        type: string
        docs: The text of the parent snippet that this chunk corresponds to.
      transcribed_text:
        type: optional<string>
        docs: >-
          The transcribed text of the generated audio of the parent snippet that
          this chunk corresponds to. It is only present if `instant_mode` is set
          to `false`.
      chunk_index:
        type: integer
        docs: The index of the audio chunk in the snippet.
      audio:
        type: string
        docs: The generated audio output chunk in the requested format.
      audio_format:
        type: AudioFormatType
        docs: The generated audio output format.
      is_last_chunk:
        type: boolean
        docs: >-
          Whether or not this is the last chunk streamed back from the decoder
          for one input snippet.
      utterance_index:
        type: optional<integer>
        docs: >-
          The index of the utterance in the request that the parent snippet of
          this chunk corresponds to.
      snippet: optional<Snippet>
    source:
      openapi: tts-openapi.json
  Timestamp:
    properties:
      type:
        type: TimestampType
      text: string
      time:
        type: MillisecondInterval
    source:
      openapi: tts-openapi.json
  TimestampType:
    enum:
      - word
      - phoneme
    source:
      openapi: tts-openapi.json
  OctaveVersion:
    enum:
      - value: '1'
        name: One
      - value: '2'
        name: Two
    docs: >-
      Selects the Octave model version used to synthesize speech for this
      request. If you omit this field, Hume automatically routes the request to
      the most appropriate model. Setting a specific version ensures stable and
      repeatable behavior across requests.


      Use `2` to opt into the latest Octave capabilities. When you specify
      version `2`, you must also provide a `voice`. Requests that set `version:
      2` without a voice will be rejected.


      For a comparison of Octave versions, see the [Octave
      versions](/docs/text-to-speech-tts/overview#octave-versions) section in
      the TTS overview.
    source:
      openapi: tts-openapi.json
  TtsOutput:
    discriminated: false
    union:
      - type: TimestampMessage
      - type: SnippetAudioChunk
    source:
      openapi: tts-openapi.json
  Snippet:
    properties:
      id:
        type: string
        docs: A unique ID associated with this **Snippet**.
      text:
        type: string
        docs: The text for this **Snippet**.
      generation_id:
        type: string
        docs: The generation ID this snippet corresponds to.
      utterance_index:
        type: optional<integer>
        docs: The index of the utterance in the request this snippet corresponds to.
      timestamps:
        docs: A list of word or phoneme level timestamps for the generated audio.
        type: list<Timestamp>
      transcribed_text:
        type: optional<string>
        docs: >-
          The transcribed text of the generated audio. It is only present if
          `instant_mode` is set to `false`.
      audio:
        type: string
        docs: >-
          The segmented audio output in the requested format, encoded as a
          base64 string.
    source:
      openapi: tts-openapi.json
  PostedContextWithGenerationId:
    properties:
      generation_id:
        type: string
        docs: >-
          The ID of a prior TTS generation to use as context for generating
          consistent speech style and prosody across multiple requests.
          Including context may increase audio generation times.
    source:
      openapi: tts-openapi.json
  PostedContextWithUtterances:
    properties:
      utterances:
        type: list<PostedUtterance>
    source:
      openapi: tts-openapi.json
  AudioEncoding:
    docs: >-
      Encoding information about the generated audio, including the `format` and
      `sample_rate`.
    properties:
      format:
        type: AudioFormatType
        docs: Format for the output audio.
      sample_rate:
        type: integer
        docs: >-
          The sample rate (`Hz`) of the generated audio. The default sample rate
          is `48000 Hz`.
    source:
      openapi: tts-openapi.json
  ReturnGeneration:
    properties:
      generation_id:
        type: string
        docs: >-
          A unique ID associated with this TTS generation that can be used as
          context for generating consistent speech style and prosody across
          multiple requests.
      duration:
        type: double
        docs: Duration of the generated audio in seconds.
      file_size:
        type: integer
        docs: Size of the generated audio in bytes.
      encoding:
        type: AudioEncoding
      audio:
        type: string
        docs: >-
          The generated audio output in the requested format, encoded as a
          base64 string.
      snippets:
        docs: >-
          A list of snippet groups where each group corresponds to an utterance
          in the request. Each group contains segmented snippets that represent
          the original utterance divided into more natural-sounding units
          optimized for speech delivery.
        type: list<list<Snippet>>
    source:
      openapi: tts-openapi.json
  HTTPValidationError:
    properties:
      detail:
        type: optional<list<ValidationError>>
    source:
      openapi: tts-openapi.json
  FormatMp3:
    properties:
      type: literal<"mp3">
    source:
      openapi: tts-openapi.json
  PostedContext:
    discriminated: false
    docs: >-
      Utterances to use as context for generating consistent speech style and
      prosody across multiple requests. These will not be converted to speech
      output.
    union:
      - type: PostedContextWithGenerationId
      - type: PostedContextWithUtterances
    source:
      openapi: tts-openapi.json
    inline: true
  Format:
    discriminated: false
    docs: Specifies the output audio file format.
    union:
      - type: FormatMp3
      - type: FormatPcm
      - type: FormatWav
    source:
      openapi: tts-openapi.json
    inline: true
  PostedTts:
    properties:
      version:
        type: optional<OctaveVersion>
        docs: >-
          Selects the Octave model version used to synthesize speech for this
          request. If you omit this field, Hume automatically routes the request
          to the most appropriate model. Setting a specific version ensures
          stable and repeatable behavior across requests.


          Use `2` to opt into the latest Octave capabilities. When you specify
          version `2`, you must also provide a `voice`. Requests that set
          `version: 2` without a voice will be rejected.


          For a comparison of Octave versions, see the [Octave
          versions](/docs/text-to-speech-tts/overview#octave-versions) section
          in the TTS overview.
      context:
        type: optional<PostedContext>
        docs: >-
          Utterances to use as context for generating consistent speech style
          and prosody across multiple requests. These will not be converted to
          speech output.
      utterances:
        docs: >-
          A list of **Utterances** to be converted to speech output.


          An **Utterance** is a unit of input for
          [Octave](/docs/text-to-speech-tts/overview), and includes input
          `text`, an optional `description` to serve as the prompt for how the
          speech should be delivered, an optional `voice` specification, and
          additional controls to guide delivery for `speed` and
          `trailing_silence`.
        type: list<PostedUtterance>
      num_generations:
        type: optional<integer>
        docs: >-
          Number of audio generations to produce from the input utterances.


          Using `num_generations` enables faster processing than issuing
          multiple sequential requests. Additionally, specifying
          `num_generations` allows prosody continuation across all generations
          without repeating context, ensuring each generation sounds slightly
          different while maintaining contextual consistency.
        default: 1
        validation:
          min: 1
          max: 5
      format:
        type: optional<Format>
        docs: Specifies the output audio file format.
      split_utterances:
        type: optional<boolean>
        docs: >-
          Controls how audio output is segmented in the response.


          - When **enabled** (`true`), input utterances are automatically split
          into natural-sounding speech segments.


          - When **disabled** (`false`), the response maintains a strict
          one-to-one mapping between input utterances and output snippets. 


          This setting affects how the `snippets` array is structured in the
          response, which may be important for applications that need to track
          the relationship between input text and generated audio segments. When
          setting to `false`, avoid including utterances with long `text`, as
          this can result in distorted output.
        default: true
      strip_headers:
        type: optional<boolean>
        docs: >-
          If enabled, the audio for all the chunks of a generation, once
          concatenated together, will constitute a single audio file. Otherwise,
          if disabled, each chunk's audio will be its own audio file, each with
          its own headers (if applicable).
        default: false
      include_timestamp_types:
        type: optional<list<TimestampType>>
        docs: The set of timestamp types to include in the response.
      instant_mode:
        type: optional<boolean>
        docs: >-
          Enables ultra-low latency streaming, significantly reducing the time
          until the first audio chunk is received. Recommended for real-time
          applications requiring immediate audio playback. For further details,
          see our documentation on [instant
          mode](/docs/text-to-speech-tts/overview#ultra-low-latency-streaming-instant-mode). 

          - A
          [voice](/reference/text-to-speech-tts/synthesize-json-streaming#request.body.utterances.voice)
          must be specified when instant mode is enabled. Dynamic voice
          generation is not supported with this mode.

          - Instant mode is only supported for streaming endpoints (e.g.,
          [/v0/tts/stream/json](/reference/text-to-speech-tts/synthesize-json-streaming),
          [/v0/tts/stream/file](/reference/text-to-speech-tts/synthesize-file-streaming)).

          - Ensure only a single generation is requested
          ([num_generations](/reference/text-to-speech-tts/synthesize-json-streaming#request.body.num_generations)
          must be `1` or omitted).
        default: true
    source:
      openapi: tts-openapi.json
  ReturnTts:
    properties:
      request_id:
        type: optional<string>
        docs: >-
          A unique ID associated with this request for tracking and
          troubleshooting. Use this ID when contacting [support](/support) for
          troubleshooting assistance.
      generations:
        type: list<ReturnGeneration>
    source:
      openapi: tts-openapi.json
  ReturnVoice:
    docs: An Octave voice available for text-to-speech
    properties:
      id:
        type: optional<string>
        docs: ID of the voice in the `Voice Library`.
      name:
        type: optional<string>
        docs: Name of the voice in the `Voice Library`.
      provider:
        type: optional<VoiceProvider>
        docs: >-
          The provider associated with the created voice.


          Voices created through this endpoint will always have the provider set
          to `CUSTOM_VOICE`, indicating a custom voice stored in your account.
      compatible_octave_models: optional<list<string>>
    source:
      openapi: tts-openapi.json
  FormatPcm:
    properties:
      type: literal<"pcm">
    source:
      openapi: tts-openapi.json
  PostedUtterance:
    properties:
      text:
        type: string
        docs: The input text to be synthesized into speech.
        validation:
          maxLength: 5000
      description:
        type: optional<string>
        docs: >-
          Natural language instructions describing how the synthesized speech
          should sound, including but not limited to tone, intonation, pacing,
          and accent.


          **This field behaves differently depending on whether a voice is
          specified**:

          - **Voice specified**: the description will serve as acting directions
          for delivery. Keep directions concise—100 characters or fewer—for best
          results. See our guide on [acting
          instructions](/docs/text-to-speech-tts/acting-instructions).

          - **Voice not specified**: the description will serve as a voice
          prompt for generating a voice. See our [prompting
          guide](/docs/text-to-speech-tts/prompting) for design tips.
        validation:
          maxLength: 1000
      voice:
        type: optional<PostedUtteranceVoice>
        docs: >-
          The `name` or `id` associated with a **Voice** from the **Voice
          Library** to be used as the speaker for this and all subsequent
          `utterances`, until the `voice` field is updated again.

           See our [voices guide](/docs/text-to-speech-tts/voices) for more details on generating and specifying **Voices**.
      speed:
        type: optional<double>
        docs: >-
          Speed multiplier for the synthesized speech. Extreme values below 0.75
          and above 1.5 may sometimes cause instability to the generated output.
        default: 1
        validation:
          min: 0.5
          max: 2
      trailing_silence:
        type: optional<double>
        docs: Duration of trailing silence (in seconds) to add to this utterance
        default: 0
        validation:
          min: 0
          max: 5
    source:
      openapi: tts-openapi.json
  ValidationErrorLocItem:
    discriminated: false
    union:
      - string
      - integer
    source:
      openapi: tts-openapi.json
    inline: true
  ValidationError:
    properties:
      loc:
        type: list<ValidationErrorLocItem>
      msg: string
      type: string
    source:
      openapi: tts-openapi.json
  FormatWav:
    properties:
      type: literal<"wav">
    source:
      openapi: tts-openapi.json
  ErrorResponse:
    properties:
      error: optional<string>
      message: optional<string>
      code: optional<string>
    source:
      openapi: tts-openapi.json
  ReturnPagedVoices:
    docs: A paginated list Octave voices available for text-to-speech
    properties:
      page_number:
        type: optional<integer>
        docs: >-
          The page number of the returned list.


          This value corresponds to the `page_number` parameter specified in the
          request. Pagination uses zero-based indexing.
      page_size:
        type: optional<integer>
        docs: >-
          The maximum number of items returned per page.


          This value corresponds to the `page_size` parameter specified in the
          request.
      total_pages:
        type: optional<integer>
        docs: The total number of pages in the collection.
      voices_page:
        type: optional<list<ReturnVoice>>
        docs: >-
          List of voices returned for the specified `page_number` and
          `page_size`.
    source:
      openapi: tts-openapi.json
