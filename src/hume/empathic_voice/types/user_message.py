# This file was auto-generated by Fern from our API Definition.

from ...core.pydantic_utilities import UniversalBaseModel
import typing
import pydantic
from .chat_message import ChatMessage
from .inference import Inference
from .millisecond_interval import MillisecondInterval
from ...core.pydantic_utilities import IS_PYDANTIC_V2


class UserMessage(UniversalBaseModel):
    """
    When provided, the output is a user message.
    """

    type: typing.Literal["user_message"] = pydantic.Field(default="user_message")
    """
    The type of message sent through the socket; for a User Message, this must be `user_message`.
    
    This message contains both a transcript of the user’s input and the expression measurement predictions if the input was sent as an [Audio Input message](/reference/empathic-voice-interface-evi/chat/chat#send.Audio%20Input.type). Expression measurement predictions are not provided for a [User Input message](/reference/empathic-voice-interface-evi/chat/chat#send.User%20Input.type), as the prosody model relies on audio input and cannot process text alone.
    """

    custom_session_id: typing.Optional[str] = pydantic.Field(default=None)
    """
    Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.
    """

    message: ChatMessage = pydantic.Field()
    """
    Transcript of the message.
    """

    models: Inference = pydantic.Field()
    """
    Inference model results.
    """

    time: MillisecondInterval = pydantic.Field()
    """
    Start and End time of user message.
    """

    from_text: bool = pydantic.Field()
    """
    Indicates if this message was inserted into the conversation as text from a [User Input](/reference/empathic-voice-interface-evi/chat/chat#send.User%20Input.text) message.
    """

    interim: bool = pydantic.Field()
    """
    Indicates if this message contains an immediate and unfinalized transcript of the user’s audio input. If it does, words may be repeated across successive `UserMessage` messages as our transcription model becomes more confident about what was said with additional context. Interim messages are useful to detect if the user is interrupting during audio playback on the client. Even without a finalized transcription, along with [UserInterrupt](/reference/empathic-voice-interface-evi/chat/chat#receive.User%20Interruption.type) messages, interim `UserMessages` are useful for detecting if the user is interrupting during audio playback on the client, signaling to stop playback in your application. Interim `UserMessages` will only be received if the [verbose_transcription](/reference/empathic-voice-interface-evi/chat/chat#request.query.verbose_transcription) query parameter is set to `true` in the handshake request.
    """

    if IS_PYDANTIC_V2:
        model_config: typing.ClassVar[pydantic.ConfigDict] = pydantic.ConfigDict(extra="allow", frozen=True)  # type: ignore # Pydantic v2
    else:

        class Config:
            frozen = True
            smart_union = True
            extra = pydantic.Extra.allow
