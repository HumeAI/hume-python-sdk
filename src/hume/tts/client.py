# This file was auto-generated by Fern from our API Definition.

from __future__ import annotations

import typing

from .. import core
from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.request_options import RequestOptions
from .raw_client import AsyncRawTtsClient, RawTtsClient
from .types.format import Format
from .types.octave_version import OctaveVersion
from .types.posted_context import PostedContext
from .types.posted_utterance import PostedUtterance
from .types.posted_utterance_voice import PostedUtteranceVoice
from .types.return_tts import ReturnTts
from .types.timestamp_type import TimestampType
from .types.tts_output import TtsOutput

if typing.TYPE_CHECKING:
    from .stream_input.client import AsyncStreamInputClient, StreamInputClient
    from .voices.client import AsyncVoicesClient, VoicesClient
# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class TtsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawTtsClient(client_wrapper=client_wrapper)
        self._client_wrapper = client_wrapper
        self._voices: typing.Optional[VoicesClient] = None
        self._stream_input: typing.Optional[StreamInputClient] = None

    @property
    def with_raw_response(self) -> RawTtsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawTtsClient
        """
        return self._raw_client

    def synthesize_json(
        self,
        *,
        utterances: typing.Sequence[PostedUtterance],
        context: typing.Optional[PostedContext] = OMIT,
        format: typing.Optional[Format] = OMIT,
        include_timestamp_types: typing.Optional[typing.Sequence[TimestampType]] = OMIT,
        num_generations: typing.Optional[int] = OMIT,
        split_utterances: typing.Optional[bool] = OMIT,
        strip_headers: typing.Optional[bool] = OMIT,
        version: typing.Optional[OctaveVersion] = OMIT,
        instant_mode: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ReturnTts:
        """
        Synthesizes one or more input texts into speech using the specified voice. If no voice is provided, a novel voice will be generated dynamically. Optionally, additional context can be included to influence the speech's style and prosody.

        The response includes the base64-encoded audio and metadata in JSON format.

        Parameters
        ----------
        utterances : typing.Sequence[PostedUtterance]
            A list of **Utterances** to be converted to speech output.

            An **Utterance** is a unit of input for [Octave](/docs/text-to-speech-tts/overview), and includes input `text`, an optional `description` to serve as the prompt for how the speech should be delivered, an optional `voice` specification, and additional controls to guide delivery for `speed` and `trailing_silence`.

        context : typing.Optional[PostedContext]
            Utterances to use as context for generating consistent speech style and prosody across multiple requests. These will not be converted to speech output.

        format : typing.Optional[Format]
            Specifies the output audio file format.

        include_timestamp_types : typing.Optional[typing.Sequence[TimestampType]]
            The set of timestamp types to include in the response. Only supported for Octave 2 requests.

        num_generations : typing.Optional[int]
            Number of audio generations to produce from the input utterances.

            Using `num_generations` enables faster processing than issuing multiple sequential requests. Additionally, specifying `num_generations` allows prosody continuation across all generations without repeating context, ensuring each generation sounds slightly different while maintaining contextual consistency.

        split_utterances : typing.Optional[bool]
            Controls how audio output is segmented in the response.

            - When **enabled** (`true`), input utterances are automatically split into natural-sounding speech segments.

            - When **disabled** (`false`), the response maintains a strict one-to-one mapping between input utterances and output snippets.

            This setting affects how the `snippets` array is structured in the response, which may be important for applications that need to track the relationship between input text and generated audio segments. When setting to `false`, avoid including utterances with long `text`, as this can result in distorted output.

        strip_headers : typing.Optional[bool]
            If enabled, the audio for all the chunks of a generation, once concatenated together, will constitute a single audio file. Otherwise, if disabled, each chunk's audio will be its own audio file, each with its own headers (if applicable).

        version : typing.Optional[OctaveVersion]
            Selects the Octave model version used to synthesize speech for this request. If you omit this field, Hume automatically routes the request to the most appropriate model. Setting a specific version ensures stable and repeatable behavior across requests.

            Use `2` to opt into the latest Octave capabilities. When you specify version `2`, you must also provide a `voice`. Requests that set `version: 2` without a voice will be rejected.

            For a comparison of Octave versions, see the [Octave versions](/docs/text-to-speech-tts/overview#octave-versions) section in the TTS overview.

        instant_mode : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ReturnTts
            Successful Response

        Examples
        --------
        from hume import HumeClient
        from hume.tts import FormatMp3, PostedContextWithUtterances, PostedUtterance

        client = HumeClient(
            api_key="YOUR_API_KEY",
        )
        client.tts.synthesize_json(
            context=PostedContextWithUtterances(
                utterances=[
                    PostedUtterance(
                        text="How can people see beauty so differently?",
                        description="A curious student with a clear and respectful tone, seeking clarification on Hume's ideas with a straightforward question.",
                    )
                ],
            ),
            format=FormatMp3(),
            num_generations=1,
            utterances=[
                PostedUtterance(
                    text="Beauty is no quality in things themselves: It exists merely in the mind which contemplates them.",
                    description="Middle-aged masculine voice with a clear, rhythmic Scots lilt, rounded vowels, and a warm, steady tone with an articulate, academic quality.",
                )
            ],
        )
        """
        _response = self._raw_client.synthesize_json(
            utterances=utterances,
            context=context,
            format=format,
            include_timestamp_types=include_timestamp_types,
            num_generations=num_generations,
            split_utterances=split_utterances,
            strip_headers=strip_headers,
            version=version,
            instant_mode=instant_mode,
            request_options=request_options,
        )
        return _response.data

    def synthesize_file(
        self,
        *,
        utterances: typing.Sequence[PostedUtterance],
        context: typing.Optional[PostedContext] = OMIT,
        format: typing.Optional[Format] = OMIT,
        include_timestamp_types: typing.Optional[typing.Sequence[TimestampType]] = OMIT,
        num_generations: typing.Optional[int] = OMIT,
        split_utterances: typing.Optional[bool] = OMIT,
        strip_headers: typing.Optional[bool] = OMIT,
        version: typing.Optional[OctaveVersion] = OMIT,
        instant_mode: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Iterator[bytes]:
        """
        Synthesizes one or more input texts into speech using the specified voice. If no voice is provided, a novel voice will be generated dynamically. Optionally, additional context can be included to influence the speech's style and prosody.

        The response contains the generated audio file in the requested format.

        Parameters
        ----------
        utterances : typing.Sequence[PostedUtterance]
            A list of **Utterances** to be converted to speech output.

            An **Utterance** is a unit of input for [Octave](/docs/text-to-speech-tts/overview), and includes input `text`, an optional `description` to serve as the prompt for how the speech should be delivered, an optional `voice` specification, and additional controls to guide delivery for `speed` and `trailing_silence`.

        context : typing.Optional[PostedContext]
            Utterances to use as context for generating consistent speech style and prosody across multiple requests. These will not be converted to speech output.

        format : typing.Optional[Format]
            Specifies the output audio file format.

        include_timestamp_types : typing.Optional[typing.Sequence[TimestampType]]
            The set of timestamp types to include in the response. Only supported for Octave 2 requests.

        num_generations : typing.Optional[int]
            Number of audio generations to produce from the input utterances.

            Using `num_generations` enables faster processing than issuing multiple sequential requests. Additionally, specifying `num_generations` allows prosody continuation across all generations without repeating context, ensuring each generation sounds slightly different while maintaining contextual consistency.

        split_utterances : typing.Optional[bool]
            Controls how audio output is segmented in the response.

            - When **enabled** (`true`), input utterances are automatically split into natural-sounding speech segments.

            - When **disabled** (`false`), the response maintains a strict one-to-one mapping between input utterances and output snippets.

            This setting affects how the `snippets` array is structured in the response, which may be important for applications that need to track the relationship between input text and generated audio segments. When setting to `false`, avoid including utterances with long `text`, as this can result in distorted output.

        strip_headers : typing.Optional[bool]
            If enabled, the audio for all the chunks of a generation, once concatenated together, will constitute a single audio file. Otherwise, if disabled, each chunk's audio will be its own audio file, each with its own headers (if applicable).

        version : typing.Optional[OctaveVersion]
            Selects the Octave model version used to synthesize speech for this request. If you omit this field, Hume automatically routes the request to the most appropriate model. Setting a specific version ensures stable and repeatable behavior across requests.

            Use `2` to opt into the latest Octave capabilities. When you specify version `2`, you must also provide a `voice`. Requests that set `version: 2` without a voice will be rejected.

            For a comparison of Octave versions, see the [Octave versions](/docs/text-to-speech-tts/overview#octave-versions) section in the TTS overview.

        instant_mode : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration. You can pass in configuration such as `chunk_size`, and more to customize the request and response.

        Returns
        -------
        typing.Iterator[bytes]
            OK

        Examples
        --------
        from hume import HumeClient
        from hume.tts import FormatMp3, PostedContextWithGenerationId, PostedUtterance

        client = HumeClient(
            api_key="YOUR_API_KEY",
        )
        client.tts.synthesize_file(
            context=PostedContextWithGenerationId(
                generation_id="09ad914d-8e7f-40f8-a279-e34f07f7dab2",
            ),
            format=FormatMp3(),
            num_generations=1,
            utterances=[
                PostedUtterance(
                    text="Beauty is no quality in things themselves: It exists merely in the mind which contemplates them.",
                    description="Middle-aged masculine voice with a clear, rhythmic Scots lilt, rounded vowels, and a warm, steady tone with an articulate, academic quality.",
                )
            ],
        )
        """
        with self._raw_client.synthesize_file(
            utterances=utterances,
            context=context,
            format=format,
            include_timestamp_types=include_timestamp_types,
            num_generations=num_generations,
            split_utterances=split_utterances,
            strip_headers=strip_headers,
            version=version,
            instant_mode=instant_mode,
            request_options=request_options,
        ) as r:
            yield from r.data

    def synthesize_file_streaming(
        self,
        *,
        utterances: typing.Sequence[PostedUtterance],
        context: typing.Optional[PostedContext] = OMIT,
        format: typing.Optional[Format] = OMIT,
        include_timestamp_types: typing.Optional[typing.Sequence[TimestampType]] = OMIT,
        num_generations: typing.Optional[int] = OMIT,
        split_utterances: typing.Optional[bool] = OMIT,
        strip_headers: typing.Optional[bool] = OMIT,
        version: typing.Optional[OctaveVersion] = OMIT,
        instant_mode: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Iterator[bytes]:
        """
        Streams synthesized speech using the specified voice. If no voice is provided, a novel voice will be generated dynamically. Optionally, additional context can be included to influence the speech's style and prosody.

        Parameters
        ----------
        utterances : typing.Sequence[PostedUtterance]
            A list of **Utterances** to be converted to speech output.

            An **Utterance** is a unit of input for [Octave](/docs/text-to-speech-tts/overview), and includes input `text`, an optional `description` to serve as the prompt for how the speech should be delivered, an optional `voice` specification, and additional controls to guide delivery for `speed` and `trailing_silence`.

        context : typing.Optional[PostedContext]
            Utterances to use as context for generating consistent speech style and prosody across multiple requests. These will not be converted to speech output.

        format : typing.Optional[Format]
            Specifies the output audio file format.

        include_timestamp_types : typing.Optional[typing.Sequence[TimestampType]]
            The set of timestamp types to include in the response. Only supported for Octave 2 requests.

        num_generations : typing.Optional[int]
            Number of audio generations to produce from the input utterances.

            Using `num_generations` enables faster processing than issuing multiple sequential requests. Additionally, specifying `num_generations` allows prosody continuation across all generations without repeating context, ensuring each generation sounds slightly different while maintaining contextual consistency.

        split_utterances : typing.Optional[bool]
            Controls how audio output is segmented in the response.

            - When **enabled** (`true`), input utterances are automatically split into natural-sounding speech segments.

            - When **disabled** (`false`), the response maintains a strict one-to-one mapping between input utterances and output snippets.

            This setting affects how the `snippets` array is structured in the response, which may be important for applications that need to track the relationship between input text and generated audio segments. When setting to `false`, avoid including utterances with long `text`, as this can result in distorted output.

        strip_headers : typing.Optional[bool]
            If enabled, the audio for all the chunks of a generation, once concatenated together, will constitute a single audio file. Otherwise, if disabled, each chunk's audio will be its own audio file, each with its own headers (if applicable).

        version : typing.Optional[OctaveVersion]
            Selects the Octave model version used to synthesize speech for this request. If you omit this field, Hume automatically routes the request to the most appropriate model. Setting a specific version ensures stable and repeatable behavior across requests.

            Use `2` to opt into the latest Octave capabilities. When you specify version `2`, you must also provide a `voice`. Requests that set `version: 2` without a voice will be rejected.

            For a comparison of Octave versions, see the [Octave versions](/docs/text-to-speech-tts/overview#octave-versions) section in the TTS overview.

        instant_mode : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration. You can pass in configuration such as `chunk_size`, and more to customize the request and response.

        Returns
        -------
        typing.Iterator[bytes]
            OK

        Examples
        --------
        from hume import HumeClient
        from hume.tts import PostedUtterance, PostedUtteranceVoiceWithName

        client = HumeClient(
            api_key="YOUR_API_KEY",
        )
        client.tts.synthesize_file_streaming(
            utterances=[
                PostedUtterance(
                    text="Beauty is no quality in things themselves: It exists merely in the mind which contemplates them.",
                    voice=PostedUtteranceVoiceWithName(
                        name="Male English Actor",
                        provider="HUME_AI",
                    ),
                )
            ],
        )
        """
        with self._raw_client.synthesize_file_streaming(
            utterances=utterances,
            context=context,
            format=format,
            include_timestamp_types=include_timestamp_types,
            num_generations=num_generations,
            split_utterances=split_utterances,
            strip_headers=strip_headers,
            version=version,
            instant_mode=instant_mode,
            request_options=request_options,
        ) as r:
            yield from r.data

    def synthesize_json_streaming(
        self,
        *,
        utterances: typing.Sequence[PostedUtterance],
        context: typing.Optional[PostedContext] = OMIT,
        format: typing.Optional[Format] = OMIT,
        include_timestamp_types: typing.Optional[typing.Sequence[TimestampType]] = OMIT,
        num_generations: typing.Optional[int] = OMIT,
        split_utterances: typing.Optional[bool] = OMIT,
        strip_headers: typing.Optional[bool] = OMIT,
        version: typing.Optional[OctaveVersion] = OMIT,
        instant_mode: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Iterator[TtsOutput]:
        """
        Streams synthesized speech using the specified voice. If no voice is provided, a novel voice will be generated dynamically. Optionally, additional context can be included to influence the speech's style and prosody.

        The response is a stream of JSON objects including audio encoded in base64.

        Parameters
        ----------
        utterances : typing.Sequence[PostedUtterance]
            A list of **Utterances** to be converted to speech output.

            An **Utterance** is a unit of input for [Octave](/docs/text-to-speech-tts/overview), and includes input `text`, an optional `description` to serve as the prompt for how the speech should be delivered, an optional `voice` specification, and additional controls to guide delivery for `speed` and `trailing_silence`.

        context : typing.Optional[PostedContext]
            Utterances to use as context for generating consistent speech style and prosody across multiple requests. These will not be converted to speech output.

        format : typing.Optional[Format]
            Specifies the output audio file format.

        include_timestamp_types : typing.Optional[typing.Sequence[TimestampType]]
            The set of timestamp types to include in the response. Only supported for Octave 2 requests.

        num_generations : typing.Optional[int]
            Number of audio generations to produce from the input utterances.

            Using `num_generations` enables faster processing than issuing multiple sequential requests. Additionally, specifying `num_generations` allows prosody continuation across all generations without repeating context, ensuring each generation sounds slightly different while maintaining contextual consistency.

        split_utterances : typing.Optional[bool]
            Controls how audio output is segmented in the response.

            - When **enabled** (`true`), input utterances are automatically split into natural-sounding speech segments.

            - When **disabled** (`false`), the response maintains a strict one-to-one mapping between input utterances and output snippets.

            This setting affects how the `snippets` array is structured in the response, which may be important for applications that need to track the relationship between input text and generated audio segments. When setting to `false`, avoid including utterances with long `text`, as this can result in distorted output.

        strip_headers : typing.Optional[bool]
            If enabled, the audio for all the chunks of a generation, once concatenated together, will constitute a single audio file. Otherwise, if disabled, each chunk's audio will be its own audio file, each with its own headers (if applicable).

        version : typing.Optional[OctaveVersion]
            Selects the Octave model version used to synthesize speech for this request. If you omit this field, Hume automatically routes the request to the most appropriate model. Setting a specific version ensures stable and repeatable behavior across requests.

            Use `2` to opt into the latest Octave capabilities. When you specify version `2`, you must also provide a `voice`. Requests that set `version: 2` without a voice will be rejected.

            For a comparison of Octave versions, see the [Octave versions](/docs/text-to-speech-tts/overview#octave-versions) section in the TTS overview.

        instant_mode : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[TtsOutput]
            Successful Response

        Examples
        --------
        from hume import HumeClient
        from hume.tts import PostedUtterance, PostedUtteranceVoiceWithName

        client = HumeClient(
            api_key="YOUR_API_KEY",
        )
        response = client.tts.synthesize_json_streaming(
            utterances=[
                PostedUtterance(
                    text="Beauty is no quality in things themselves: It exists merely in the mind which contemplates them.",
                    voice=PostedUtteranceVoiceWithName(
                        name="Male English Actor",
                        provider="HUME_AI",
                    ),
                )
            ],
        )
        for chunk in response:
            yield chunk
        """
        with self._raw_client.synthesize_json_streaming(
            utterances=utterances,
            context=context,
            format=format,
            include_timestamp_types=include_timestamp_types,
            num_generations=num_generations,
            split_utterances=split_utterances,
            strip_headers=strip_headers,
            version=version,
            instant_mode=instant_mode,
            request_options=request_options,
        ) as r:
            yield from r.data

    def convert_voice_file(
        self,
        *,
        audio: core.File,
        strip_headers: typing.Optional[bool] = OMIT,
        context: typing.Optional[PostedContext] = OMIT,
        voice: typing.Optional[PostedUtteranceVoice] = OMIT,
        format: typing.Optional[Format] = OMIT,
        include_timestamp_types: typing.Optional[typing.List[TimestampType]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Iterator[bytes]:
        """
        Parameters
        ----------
        audio : core.File
            See core.File for more documentation

        strip_headers : typing.Optional[bool]
            If enabled, the audio for all the chunks of a generation, once concatenated together, will constitute a single audio file. Otherwise, if disabled, each chunk's audio will be its own audio file, each with its own headers (if applicable).

        context : typing.Optional[PostedContext]
            Utterances to use as context for generating consistent speech style and prosody across multiple requests. These will not be converted to speech output.

        voice : typing.Optional[PostedUtteranceVoice]

        format : typing.Optional[Format]
            Specifies the output audio file format.

        include_timestamp_types : typing.Optional[typing.List[TimestampType]]
            The set of timestamp types to include in the response. When used in multipart/form-data, specify each value using bracket notation: `include_timestamp_types[0]=word&include_timestamp_types[1]=phoneme`. Only supported for Octave 2 requests.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration. You can pass in configuration such as `chunk_size`, and more to customize the request and response.

        Returns
        -------
        typing.Iterator[bytes]
            Successful Response
        """
        with self._raw_client.convert_voice_file(
            audio=audio,
            strip_headers=strip_headers,
            context=context,
            voice=voice,
            format=format,
            include_timestamp_types=include_timestamp_types,
            request_options=request_options,
        ) as r:
            yield from r.data

    def convert_voice_json(
        self,
        *,
        strip_headers: typing.Optional[bool] = OMIT,
        audio: typing.Optional[core.File] = OMIT,
        context: typing.Optional[PostedContext] = OMIT,
        voice: typing.Optional[PostedUtteranceVoice] = OMIT,
        format: typing.Optional[Format] = OMIT,
        include_timestamp_types: typing.Optional[typing.List[TimestampType]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Iterator[TtsOutput]:
        """
        Parameters
        ----------
        strip_headers : typing.Optional[bool]
            If enabled, the audio for all the chunks of a generation, once concatenated together, will constitute a single audio file. Otherwise, if disabled, each chunk's audio will be its own audio file, each with its own headers (if applicable).

        audio : typing.Optional[core.File]
            See core.File for more documentation

        context : typing.Optional[PostedContext]
            Utterances to use as context for generating consistent speech style and prosody across multiple requests. These will not be converted to speech output.

        voice : typing.Optional[PostedUtteranceVoice]

        format : typing.Optional[Format]
            Specifies the output audio file format.

        include_timestamp_types : typing.Optional[typing.List[TimestampType]]
            The set of timestamp types to include in the response. When used in multipart/form-data, specify each value using bracket notation: `include_timestamp_types[0]=word&include_timestamp_types[1]=phoneme`. Only supported for Octave 2 requests.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[TtsOutput]
            Successful Response

        Examples
        --------
        from hume import HumeClient

        client = HumeClient(
            api_key="YOUR_API_KEY",
        )
        response = client.tts.convert_voice_json()
        for chunk in response:
            yield chunk
        """
        with self._raw_client.convert_voice_json(
            strip_headers=strip_headers,
            audio=audio,
            context=context,
            voice=voice,
            format=format,
            include_timestamp_types=include_timestamp_types,
            request_options=request_options,
        ) as r:
            yield from r.data

    @property
    def voices(self):
        if self._voices is None:
            from .voices.client import VoicesClient  # noqa: E402

            self._voices = VoicesClient(client_wrapper=self._client_wrapper)
        return self._voices

    @property
    def stream_input(self):
        if self._stream_input is None:
            from .stream_input.client import StreamInputClient  # noqa: E402

            self._stream_input = StreamInputClient(client_wrapper=self._client_wrapper)
        return self._stream_input


class AsyncTtsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawTtsClient(client_wrapper=client_wrapper)
        self._client_wrapper = client_wrapper
        self._voices: typing.Optional[AsyncVoicesClient] = None
        self._stream_input: typing.Optional[AsyncStreamInputClient] = None

    @property
    def with_raw_response(self) -> AsyncRawTtsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawTtsClient
        """
        return self._raw_client

    async def synthesize_json(
        self,
        *,
        utterances: typing.Sequence[PostedUtterance],
        context: typing.Optional[PostedContext] = OMIT,
        format: typing.Optional[Format] = OMIT,
        include_timestamp_types: typing.Optional[typing.Sequence[TimestampType]] = OMIT,
        num_generations: typing.Optional[int] = OMIT,
        split_utterances: typing.Optional[bool] = OMIT,
        strip_headers: typing.Optional[bool] = OMIT,
        version: typing.Optional[OctaveVersion] = OMIT,
        instant_mode: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ReturnTts:
        """
        Synthesizes one or more input texts into speech using the specified voice. If no voice is provided, a novel voice will be generated dynamically. Optionally, additional context can be included to influence the speech's style and prosody.

        The response includes the base64-encoded audio and metadata in JSON format.

        Parameters
        ----------
        utterances : typing.Sequence[PostedUtterance]
            A list of **Utterances** to be converted to speech output.

            An **Utterance** is a unit of input for [Octave](/docs/text-to-speech-tts/overview), and includes input `text`, an optional `description` to serve as the prompt for how the speech should be delivered, an optional `voice` specification, and additional controls to guide delivery for `speed` and `trailing_silence`.

        context : typing.Optional[PostedContext]
            Utterances to use as context for generating consistent speech style and prosody across multiple requests. These will not be converted to speech output.

        format : typing.Optional[Format]
            Specifies the output audio file format.

        include_timestamp_types : typing.Optional[typing.Sequence[TimestampType]]
            The set of timestamp types to include in the response. Only supported for Octave 2 requests.

        num_generations : typing.Optional[int]
            Number of audio generations to produce from the input utterances.

            Using `num_generations` enables faster processing than issuing multiple sequential requests. Additionally, specifying `num_generations` allows prosody continuation across all generations without repeating context, ensuring each generation sounds slightly different while maintaining contextual consistency.

        split_utterances : typing.Optional[bool]
            Controls how audio output is segmented in the response.

            - When **enabled** (`true`), input utterances are automatically split into natural-sounding speech segments.

            - When **disabled** (`false`), the response maintains a strict one-to-one mapping between input utterances and output snippets.

            This setting affects how the `snippets` array is structured in the response, which may be important for applications that need to track the relationship between input text and generated audio segments. When setting to `false`, avoid including utterances with long `text`, as this can result in distorted output.

        strip_headers : typing.Optional[bool]
            If enabled, the audio for all the chunks of a generation, once concatenated together, will constitute a single audio file. Otherwise, if disabled, each chunk's audio will be its own audio file, each with its own headers (if applicable).

        version : typing.Optional[OctaveVersion]
            Selects the Octave model version used to synthesize speech for this request. If you omit this field, Hume automatically routes the request to the most appropriate model. Setting a specific version ensures stable and repeatable behavior across requests.

            Use `2` to opt into the latest Octave capabilities. When you specify version `2`, you must also provide a `voice`. Requests that set `version: 2` without a voice will be rejected.

            For a comparison of Octave versions, see the [Octave versions](/docs/text-to-speech-tts/overview#octave-versions) section in the TTS overview.

        instant_mode : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ReturnTts
            Successful Response

        Examples
        --------
        import asyncio

        from hume import AsyncHumeClient
        from hume.tts import FormatMp3, PostedContextWithUtterances, PostedUtterance

        client = AsyncHumeClient(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.tts.synthesize_json(
                context=PostedContextWithUtterances(
                    utterances=[
                        PostedUtterance(
                            text="How can people see beauty so differently?",
                            description="A curious student with a clear and respectful tone, seeking clarification on Hume's ideas with a straightforward question.",
                        )
                    ],
                ),
                format=FormatMp3(),
                num_generations=1,
                utterances=[
                    PostedUtterance(
                        text="Beauty is no quality in things themselves: It exists merely in the mind which contemplates them.",
                        description="Middle-aged masculine voice with a clear, rhythmic Scots lilt, rounded vowels, and a warm, steady tone with an articulate, academic quality.",
                    )
                ],
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.synthesize_json(
            utterances=utterances,
            context=context,
            format=format,
            include_timestamp_types=include_timestamp_types,
            num_generations=num_generations,
            split_utterances=split_utterances,
            strip_headers=strip_headers,
            version=version,
            instant_mode=instant_mode,
            request_options=request_options,
        )
        return _response.data

    async def synthesize_file(
        self,
        *,
        utterances: typing.Sequence[PostedUtterance],
        context: typing.Optional[PostedContext] = OMIT,
        format: typing.Optional[Format] = OMIT,
        include_timestamp_types: typing.Optional[typing.Sequence[TimestampType]] = OMIT,
        num_generations: typing.Optional[int] = OMIT,
        split_utterances: typing.Optional[bool] = OMIT,
        strip_headers: typing.Optional[bool] = OMIT,
        version: typing.Optional[OctaveVersion] = OMIT,
        instant_mode: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.AsyncIterator[bytes]:
        """
        Synthesizes one or more input texts into speech using the specified voice. If no voice is provided, a novel voice will be generated dynamically. Optionally, additional context can be included to influence the speech's style and prosody.

        The response contains the generated audio file in the requested format.

        Parameters
        ----------
        utterances : typing.Sequence[PostedUtterance]
            A list of **Utterances** to be converted to speech output.

            An **Utterance** is a unit of input for [Octave](/docs/text-to-speech-tts/overview), and includes input `text`, an optional `description` to serve as the prompt for how the speech should be delivered, an optional `voice` specification, and additional controls to guide delivery for `speed` and `trailing_silence`.

        context : typing.Optional[PostedContext]
            Utterances to use as context for generating consistent speech style and prosody across multiple requests. These will not be converted to speech output.

        format : typing.Optional[Format]
            Specifies the output audio file format.

        include_timestamp_types : typing.Optional[typing.Sequence[TimestampType]]
            The set of timestamp types to include in the response. Only supported for Octave 2 requests.

        num_generations : typing.Optional[int]
            Number of audio generations to produce from the input utterances.

            Using `num_generations` enables faster processing than issuing multiple sequential requests. Additionally, specifying `num_generations` allows prosody continuation across all generations without repeating context, ensuring each generation sounds slightly different while maintaining contextual consistency.

        split_utterances : typing.Optional[bool]
            Controls how audio output is segmented in the response.

            - When **enabled** (`true`), input utterances are automatically split into natural-sounding speech segments.

            - When **disabled** (`false`), the response maintains a strict one-to-one mapping between input utterances and output snippets.

            This setting affects how the `snippets` array is structured in the response, which may be important for applications that need to track the relationship between input text and generated audio segments. When setting to `false`, avoid including utterances with long `text`, as this can result in distorted output.

        strip_headers : typing.Optional[bool]
            If enabled, the audio for all the chunks of a generation, once concatenated together, will constitute a single audio file. Otherwise, if disabled, each chunk's audio will be its own audio file, each with its own headers (if applicable).

        version : typing.Optional[OctaveVersion]
            Selects the Octave model version used to synthesize speech for this request. If you omit this field, Hume automatically routes the request to the most appropriate model. Setting a specific version ensures stable and repeatable behavior across requests.

            Use `2` to opt into the latest Octave capabilities. When you specify version `2`, you must also provide a `voice`. Requests that set `version: 2` without a voice will be rejected.

            For a comparison of Octave versions, see the [Octave versions](/docs/text-to-speech-tts/overview#octave-versions) section in the TTS overview.

        instant_mode : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration. You can pass in configuration such as `chunk_size`, and more to customize the request and response.

        Returns
        -------
        typing.AsyncIterator[bytes]
            OK

        Examples
        --------
        import asyncio

        from hume import AsyncHumeClient
        from hume.tts import FormatMp3, PostedContextWithGenerationId, PostedUtterance

        client = AsyncHumeClient(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.tts.synthesize_file(
                context=PostedContextWithGenerationId(
                    generation_id="09ad914d-8e7f-40f8-a279-e34f07f7dab2",
                ),
                format=FormatMp3(),
                num_generations=1,
                utterances=[
                    PostedUtterance(
                        text="Beauty is no quality in things themselves: It exists merely in the mind which contemplates them.",
                        description="Middle-aged masculine voice with a clear, rhythmic Scots lilt, rounded vowels, and a warm, steady tone with an articulate, academic quality.",
                    )
                ],
            )


        asyncio.run(main())
        """
        async with self._raw_client.synthesize_file(
            utterances=utterances,
            context=context,
            format=format,
            include_timestamp_types=include_timestamp_types,
            num_generations=num_generations,
            split_utterances=split_utterances,
            strip_headers=strip_headers,
            version=version,
            instant_mode=instant_mode,
            request_options=request_options,
        ) as r:
            async for _chunk in r.data:
                yield _chunk

    async def synthesize_file_streaming(
        self,
        *,
        utterances: typing.Sequence[PostedUtterance],
        context: typing.Optional[PostedContext] = OMIT,
        format: typing.Optional[Format] = OMIT,
        include_timestamp_types: typing.Optional[typing.Sequence[TimestampType]] = OMIT,
        num_generations: typing.Optional[int] = OMIT,
        split_utterances: typing.Optional[bool] = OMIT,
        strip_headers: typing.Optional[bool] = OMIT,
        version: typing.Optional[OctaveVersion] = OMIT,
        instant_mode: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.AsyncIterator[bytes]:
        """
        Streams synthesized speech using the specified voice. If no voice is provided, a novel voice will be generated dynamically. Optionally, additional context can be included to influence the speech's style and prosody.

        Parameters
        ----------
        utterances : typing.Sequence[PostedUtterance]
            A list of **Utterances** to be converted to speech output.

            An **Utterance** is a unit of input for [Octave](/docs/text-to-speech-tts/overview), and includes input `text`, an optional `description` to serve as the prompt for how the speech should be delivered, an optional `voice` specification, and additional controls to guide delivery for `speed` and `trailing_silence`.

        context : typing.Optional[PostedContext]
            Utterances to use as context for generating consistent speech style and prosody across multiple requests. These will not be converted to speech output.

        format : typing.Optional[Format]
            Specifies the output audio file format.

        include_timestamp_types : typing.Optional[typing.Sequence[TimestampType]]
            The set of timestamp types to include in the response. Only supported for Octave 2 requests.

        num_generations : typing.Optional[int]
            Number of audio generations to produce from the input utterances.

            Using `num_generations` enables faster processing than issuing multiple sequential requests. Additionally, specifying `num_generations` allows prosody continuation across all generations without repeating context, ensuring each generation sounds slightly different while maintaining contextual consistency.

        split_utterances : typing.Optional[bool]
            Controls how audio output is segmented in the response.

            - When **enabled** (`true`), input utterances are automatically split into natural-sounding speech segments.

            - When **disabled** (`false`), the response maintains a strict one-to-one mapping between input utterances and output snippets.

            This setting affects how the `snippets` array is structured in the response, which may be important for applications that need to track the relationship between input text and generated audio segments. When setting to `false`, avoid including utterances with long `text`, as this can result in distorted output.

        strip_headers : typing.Optional[bool]
            If enabled, the audio for all the chunks of a generation, once concatenated together, will constitute a single audio file. Otherwise, if disabled, each chunk's audio will be its own audio file, each with its own headers (if applicable).

        version : typing.Optional[OctaveVersion]
            Selects the Octave model version used to synthesize speech for this request. If you omit this field, Hume automatically routes the request to the most appropriate model. Setting a specific version ensures stable and repeatable behavior across requests.

            Use `2` to opt into the latest Octave capabilities. When you specify version `2`, you must also provide a `voice`. Requests that set `version: 2` without a voice will be rejected.

            For a comparison of Octave versions, see the [Octave versions](/docs/text-to-speech-tts/overview#octave-versions) section in the TTS overview.

        instant_mode : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration. You can pass in configuration such as `chunk_size`, and more to customize the request and response.

        Returns
        -------
        typing.AsyncIterator[bytes]
            OK

        Examples
        --------
        import asyncio

        from hume import AsyncHumeClient
        from hume.tts import PostedUtterance, PostedUtteranceVoiceWithName

        client = AsyncHumeClient(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.tts.synthesize_file_streaming(
                utterances=[
                    PostedUtterance(
                        text="Beauty is no quality in things themselves: It exists merely in the mind which contemplates them.",
                        voice=PostedUtteranceVoiceWithName(
                            name="Male English Actor",
                            provider="HUME_AI",
                        ),
                    )
                ],
            )


        asyncio.run(main())
        """
        async with self._raw_client.synthesize_file_streaming(
            utterances=utterances,
            context=context,
            format=format,
            include_timestamp_types=include_timestamp_types,
            num_generations=num_generations,
            split_utterances=split_utterances,
            strip_headers=strip_headers,
            version=version,
            instant_mode=instant_mode,
            request_options=request_options,
        ) as r:
            async for _chunk in r.data:
                yield _chunk

    async def synthesize_json_streaming(
        self,
        *,
        utterances: typing.Sequence[PostedUtterance],
        context: typing.Optional[PostedContext] = OMIT,
        format: typing.Optional[Format] = OMIT,
        include_timestamp_types: typing.Optional[typing.Sequence[TimestampType]] = OMIT,
        num_generations: typing.Optional[int] = OMIT,
        split_utterances: typing.Optional[bool] = OMIT,
        strip_headers: typing.Optional[bool] = OMIT,
        version: typing.Optional[OctaveVersion] = OMIT,
        instant_mode: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.AsyncIterator[TtsOutput]:
        """
        Streams synthesized speech using the specified voice. If no voice is provided, a novel voice will be generated dynamically. Optionally, additional context can be included to influence the speech's style and prosody.

        The response is a stream of JSON objects including audio encoded in base64.

        Parameters
        ----------
        utterances : typing.Sequence[PostedUtterance]
            A list of **Utterances** to be converted to speech output.

            An **Utterance** is a unit of input for [Octave](/docs/text-to-speech-tts/overview), and includes input `text`, an optional `description` to serve as the prompt for how the speech should be delivered, an optional `voice` specification, and additional controls to guide delivery for `speed` and `trailing_silence`.

        context : typing.Optional[PostedContext]
            Utterances to use as context for generating consistent speech style and prosody across multiple requests. These will not be converted to speech output.

        format : typing.Optional[Format]
            Specifies the output audio file format.

        include_timestamp_types : typing.Optional[typing.Sequence[TimestampType]]
            The set of timestamp types to include in the response. Only supported for Octave 2 requests.

        num_generations : typing.Optional[int]
            Number of audio generations to produce from the input utterances.

            Using `num_generations` enables faster processing than issuing multiple sequential requests. Additionally, specifying `num_generations` allows prosody continuation across all generations without repeating context, ensuring each generation sounds slightly different while maintaining contextual consistency.

        split_utterances : typing.Optional[bool]
            Controls how audio output is segmented in the response.

            - When **enabled** (`true`), input utterances are automatically split into natural-sounding speech segments.

            - When **disabled** (`false`), the response maintains a strict one-to-one mapping between input utterances and output snippets.

            This setting affects how the `snippets` array is structured in the response, which may be important for applications that need to track the relationship between input text and generated audio segments. When setting to `false`, avoid including utterances with long `text`, as this can result in distorted output.

        strip_headers : typing.Optional[bool]
            If enabled, the audio for all the chunks of a generation, once concatenated together, will constitute a single audio file. Otherwise, if disabled, each chunk's audio will be its own audio file, each with its own headers (if applicable).

        version : typing.Optional[OctaveVersion]
            Selects the Octave model version used to synthesize speech for this request. If you omit this field, Hume automatically routes the request to the most appropriate model. Setting a specific version ensures stable and repeatable behavior across requests.

            Use `2` to opt into the latest Octave capabilities. When you specify version `2`, you must also provide a `voice`. Requests that set `version: 2` without a voice will be rejected.

            For a comparison of Octave versions, see the [Octave versions](/docs/text-to-speech-tts/overview#octave-versions) section in the TTS overview.

        instant_mode : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[TtsOutput]
            Successful Response

        Examples
        --------
        import asyncio

        from hume import AsyncHumeClient
        from hume.tts import PostedUtterance, PostedUtteranceVoiceWithName

        client = AsyncHumeClient(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            response = await client.tts.synthesize_json_streaming(
                utterances=[
                    PostedUtterance(
                        text="Beauty is no quality in things themselves: It exists merely in the mind which contemplates them.",
                        voice=PostedUtteranceVoiceWithName(
                            name="Male English Actor",
                            provider="HUME_AI",
                        ),
                    )
                ],
            )
            async for chunk in response:
                yield chunk


        asyncio.run(main())
        """
        async with self._raw_client.synthesize_json_streaming(
            utterances=utterances,
            context=context,
            format=format,
            include_timestamp_types=include_timestamp_types,
            num_generations=num_generations,
            split_utterances=split_utterances,
            strip_headers=strip_headers,
            version=version,
            instant_mode=instant_mode,
            request_options=request_options,
        ) as r:
            async for _chunk in r.data:
                yield _chunk

    async def convert_voice_file(
        self,
        *,
        audio: core.File,
        strip_headers: typing.Optional[bool] = OMIT,
        context: typing.Optional[PostedContext] = OMIT,
        voice: typing.Optional[PostedUtteranceVoice] = OMIT,
        format: typing.Optional[Format] = OMIT,
        include_timestamp_types: typing.Optional[typing.List[TimestampType]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.AsyncIterator[bytes]:
        """
        Parameters
        ----------
        audio : core.File
            See core.File for more documentation

        strip_headers : typing.Optional[bool]
            If enabled, the audio for all the chunks of a generation, once concatenated together, will constitute a single audio file. Otherwise, if disabled, each chunk's audio will be its own audio file, each with its own headers (if applicable).

        context : typing.Optional[PostedContext]
            Utterances to use as context for generating consistent speech style and prosody across multiple requests. These will not be converted to speech output.

        voice : typing.Optional[PostedUtteranceVoice]

        format : typing.Optional[Format]
            Specifies the output audio file format.

        include_timestamp_types : typing.Optional[typing.List[TimestampType]]
            The set of timestamp types to include in the response. When used in multipart/form-data, specify each value using bracket notation: `include_timestamp_types[0]=word&include_timestamp_types[1]=phoneme`. Only supported for Octave 2 requests.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration. You can pass in configuration such as `chunk_size`, and more to customize the request and response.

        Returns
        -------
        typing.AsyncIterator[bytes]
            Successful Response
        """
        async with self._raw_client.convert_voice_file(
            audio=audio,
            strip_headers=strip_headers,
            context=context,
            voice=voice,
            format=format,
            include_timestamp_types=include_timestamp_types,
            request_options=request_options,
        ) as r:
            async for _chunk in r.data:
                yield _chunk

    async def convert_voice_json(
        self,
        *,
        strip_headers: typing.Optional[bool] = OMIT,
        audio: typing.Optional[core.File] = OMIT,
        context: typing.Optional[PostedContext] = OMIT,
        voice: typing.Optional[PostedUtteranceVoice] = OMIT,
        format: typing.Optional[Format] = OMIT,
        include_timestamp_types: typing.Optional[typing.List[TimestampType]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.AsyncIterator[TtsOutput]:
        """
        Parameters
        ----------
        strip_headers : typing.Optional[bool]
            If enabled, the audio for all the chunks of a generation, once concatenated together, will constitute a single audio file. Otherwise, if disabled, each chunk's audio will be its own audio file, each with its own headers (if applicable).

        audio : typing.Optional[core.File]
            See core.File for more documentation

        context : typing.Optional[PostedContext]
            Utterances to use as context for generating consistent speech style and prosody across multiple requests. These will not be converted to speech output.

        voice : typing.Optional[PostedUtteranceVoice]

        format : typing.Optional[Format]
            Specifies the output audio file format.

        include_timestamp_types : typing.Optional[typing.List[TimestampType]]
            The set of timestamp types to include in the response. When used in multipart/form-data, specify each value using bracket notation: `include_timestamp_types[0]=word&include_timestamp_types[1]=phoneme`. Only supported for Octave 2 requests.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[TtsOutput]
            Successful Response

        Examples
        --------
        import asyncio

        from hume import AsyncHumeClient

        client = AsyncHumeClient(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            response = await client.tts.convert_voice_json()
            async for chunk in response:
                yield chunk


        asyncio.run(main())
        """
        async with self._raw_client.convert_voice_json(
            strip_headers=strip_headers,
            audio=audio,
            context=context,
            voice=voice,
            format=format,
            include_timestamp_types=include_timestamp_types,
            request_options=request_options,
        ) as r:
            async for _chunk in r.data:
                yield _chunk

    @property
    def voices(self):
        if self._voices is None:
            from .voices.client import AsyncVoicesClient  # noqa: E402

            self._voices = AsyncVoicesClient(client_wrapper=self._client_wrapper)
        return self._voices

    @property
    def stream_input(self):
        if self._stream_input is None:
            from .stream_input.client import AsyncStreamInputClient  # noqa: E402

            self._stream_input = AsyncStreamInputClient(client_wrapper=self._client_wrapper)
        return self._stream_input
